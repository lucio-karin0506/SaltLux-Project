{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2절. RNN으로 영화평 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from time import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "MY_WORDS = 10000 # 사전 안의 단어 수\n",
    "MY_LENGTH = 80 # 영화평 길이, 길면 짜름, 짧으면 제로패딩\n",
    "MY_EMBED = 32 # 임베딩 차원\n",
    "MY_HIDDEN = 64 # LSTM 차원\n",
    "MY_SAMPLE = 5 # 임의의 샘플 데이터\n",
    "MY_EPOCHS = 10 # 반복 학습 수\n",
    "MY_BATCH = 200 # 매번 가져오는 데이터 수\n",
    "\n",
    "np.random.seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=MY_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 입력 데이터의 모양 (25000,)\n",
      "학습용 출력 데이터의 모양 (25000,)\n",
      "평가용 입력 데이터의 모양 (25000,)\n",
      "평가용 출력 데이터의 모양 (25000,)\n"
     ]
    }
   ],
   "source": [
    "print('학습용 입력 데이터의 모양', X_train.shape)\n",
    "print('학습용 출력 데이터의 모양', Y_train.shape)\n",
    "print('평가용 입력 데이터의 모양', X_test.shape)\n",
    "print('평가용 출력 데이터의 모양', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 120\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0]), len(set(X_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 많이 사용되는 단어\n",
    "word_to_idx['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word = dict([(val, key) for (key, val) in word_to_idx.items()])\n",
    "idx_to_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'musicians', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'landed', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'frog', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'barrel', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 영화평을 단어 영화평으로 변환\n",
    "def decoding(review_num):\n",
    "    decoded = []\n",
    "    for i in review_num:\n",
    "        word = idx_to_word[i]\n",
    "        decoded.append(word)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "print(decoding(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34704: 'fawn',\n",
       " 52009: 'tsukino',\n",
       " 52010: 'nunnery',\n",
       " 16819: 'sonja',\n",
       " 63954: 'vani',\n",
       " 1411: 'woods',\n",
       " 16118: 'spiders',\n",
       " 2348: 'hanging',\n",
       " 2292: 'woody',\n",
       " 52011: 'trawling',\n",
       " 52012: \"hold's\",\n",
       " 11310: 'comically',\n",
       " 40833: 'localized',\n",
       " 30571: 'disobeying',\n",
       " 52013: \"'royale\",\n",
       " 40834: \"harpo's\",\n",
       " 52014: 'canet',\n",
       " 19316: 'aileen',\n",
       " 52015: 'acurately',\n",
       " 52016: \"diplomat's\",\n",
       " 25245: 'rickman',\n",
       " 6749: 'arranged',\n",
       " 52017: 'rumbustious',\n",
       " 52018: 'familiarness',\n",
       " 52019: \"spider'\",\n",
       " 68807: 'hahahah',\n",
       " 52020: \"wood'\",\n",
       " 40836: 'transvestism',\n",
       " 34705: \"hangin'\",\n",
       " 2341: 'bringing',\n",
       " 40837: 'seamier',\n",
       " 34706: 'wooded',\n",
       " 52021: 'bravora',\n",
       " 16820: 'grueling',\n",
       " 1639: 'wooden',\n",
       " 16821: 'wednesday',\n",
       " 52022: \"'prix\",\n",
       " 34707: 'altagracia',\n",
       " 52023: 'circuitry',\n",
       " 11588: 'crotch',\n",
       " 57769: 'busybody',\n",
       " 52024: \"tart'n'tangy\",\n",
       " 14132: 'burgade',\n",
       " 52026: 'thrace',\n",
       " 11041: \"tom's\",\n",
       " 52028: 'snuggles',\n",
       " 29117: 'francesco',\n",
       " 52030: 'complainers',\n",
       " 52128: 'templarios',\n",
       " 40838: '272',\n",
       " 52031: '273',\n",
       " 52133: 'zaniacs',\n",
       " 34709: '275',\n",
       " 27634: 'consenting',\n",
       " 40839: 'snuggled',\n",
       " 15495: 'inanimate',\n",
       " 52033: 'uality',\n",
       " 11929: 'bronte',\n",
       " 4013: 'errors',\n",
       " 3233: 'dialogs',\n",
       " 52034: \"yomada's\",\n",
       " 34710: \"madman's\",\n",
       " 30588: 'dialoge',\n",
       " 52036: 'usenet',\n",
       " 40840: 'videodrome',\n",
       " 26341: \"kid'\",\n",
       " 52037: 'pawed',\n",
       " 30572: \"'girlfriend'\",\n",
       " 52038: \"'pleasure\",\n",
       " 52039: \"'reloaded'\",\n",
       " 40842: \"kazakos'\",\n",
       " 52040: 'rocque',\n",
       " 52041: 'mailings',\n",
       " 11930: 'brainwashed',\n",
       " 16822: 'mcanally',\n",
       " 52042: \"tom''\",\n",
       " 25246: 'kurupt',\n",
       " 21908: 'affiliated',\n",
       " 52043: 'babaganoosh',\n",
       " 40843: \"noe's\",\n",
       " 40844: 'quart',\n",
       " 362: 'kids',\n",
       " 5037: 'uplifting',\n",
       " 7096: 'controversy',\n",
       " 21909: 'kida',\n",
       " 23382: 'kidd',\n",
       " 52044: \"error'\",\n",
       " 52045: 'neurologist',\n",
       " 18513: 'spotty',\n",
       " 30573: 'cobblers',\n",
       " 9881: 'projection',\n",
       " 40845: 'fastforwarding',\n",
       " 52046: 'sters',\n",
       " 52047: \"eggar's\",\n",
       " 52048: 'etherything',\n",
       " 40846: 'gateshead',\n",
       " 34711: 'airball',\n",
       " 25247: 'unsinkable',\n",
       " 7183: 'stern',\n",
       " 52049: \"cervi's\",\n",
       " 40847: 'dnd',\n",
       " 11589: 'dna',\n",
       " 20601: 'insecurity',\n",
       " 52050: \"'reboot'\",\n",
       " 11040: 'trelkovsky',\n",
       " 52051: 'jaekel',\n",
       " 52052: 'sidebars',\n",
       " 52053: \"sforza's\",\n",
       " 17636: 'distortions',\n",
       " 52054: 'mutinies',\n",
       " 30605: 'sermons',\n",
       " 40849: '7ft',\n",
       " 52055: 'boobage',\n",
       " 52056: \"o'bannon's\",\n",
       " 23383: 'populations',\n",
       " 52057: 'chulak',\n",
       " 27636: 'mesmerize',\n",
       " 52058: 'quinnell',\n",
       " 10310: 'yahoo',\n",
       " 52060: 'meteorologist',\n",
       " 42580: 'beswick',\n",
       " 15496: 'boorman',\n",
       " 40850: 'voicework',\n",
       " 52061: \"ster'\",\n",
       " 22925: 'blustering',\n",
       " 52062: 'hj',\n",
       " 27637: 'intake',\n",
       " 5624: 'morally',\n",
       " 40852: 'jumbling',\n",
       " 52063: 'bowersock',\n",
       " 52064: \"'porky's'\",\n",
       " 16824: 'gershon',\n",
       " 40853: 'ludicrosity',\n",
       " 52065: 'coprophilia',\n",
       " 40854: 'expressively',\n",
       " 19503: \"india's\",\n",
       " 34713: \"post's\",\n",
       " 52066: 'wana',\n",
       " 5286: 'wang',\n",
       " 30574: 'wand',\n",
       " 25248: 'wane',\n",
       " 52324: 'edgeways',\n",
       " 34714: 'titanium',\n",
       " 40855: 'pinta',\n",
       " 181: 'want',\n",
       " 30575: 'pinto',\n",
       " 52068: 'whoopdedoodles',\n",
       " 21911: 'tchaikovsky',\n",
       " 2106: 'travel',\n",
       " 52069: \"'victory'\",\n",
       " 11931: 'copious',\n",
       " 22436: 'gouge',\n",
       " 52070: \"chapters'\",\n",
       " 6705: 'barbra',\n",
       " 30576: 'uselessness',\n",
       " 52071: \"wan'\",\n",
       " 27638: 'assimilated',\n",
       " 16119: 'petiot',\n",
       " 52072: 'most\\x85and',\n",
       " 3933: 'dinosaurs',\n",
       " 355: 'wrong',\n",
       " 52073: 'seda',\n",
       " 52074: 'stollen',\n",
       " 34715: 'sentencing',\n",
       " 40856: 'ouroboros',\n",
       " 40857: 'assimilates',\n",
       " 40858: 'colorfully',\n",
       " 27639: 'glenne',\n",
       " 52075: 'dongen',\n",
       " 4763: 'subplots',\n",
       " 52076: 'kiloton',\n",
       " 23384: 'chandon',\n",
       " 34716: \"effect'\",\n",
       " 27640: 'snugly',\n",
       " 40859: 'kuei',\n",
       " 9095: 'welcomed',\n",
       " 30074: 'dishonor',\n",
       " 52078: 'concurrence',\n",
       " 23385: 'stoicism',\n",
       " 14899: \"guys'\",\n",
       " 52080: \"beroemd'\",\n",
       " 6706: 'butcher',\n",
       " 40860: \"melfi's\",\n",
       " 30626: 'aargh',\n",
       " 20602: 'playhouse',\n",
       " 11311: 'wickedly',\n",
       " 1183: 'fit',\n",
       " 52081: 'labratory',\n",
       " 40862: 'lifeline',\n",
       " 1930: 'screaming',\n",
       " 4290: 'fix',\n",
       " 52082: 'cineliterate',\n",
       " 52083: 'fic',\n",
       " 52084: 'fia',\n",
       " 34717: 'fig',\n",
       " 52085: 'fmvs',\n",
       " 52086: 'fie',\n",
       " 52087: 'reentered',\n",
       " 30577: 'fin',\n",
       " 52088: 'doctresses',\n",
       " 52089: 'fil',\n",
       " 12609: 'zucker',\n",
       " 31934: 'ached',\n",
       " 52091: 'counsil',\n",
       " 52092: 'paterfamilias',\n",
       " 13888: 'songwriter',\n",
       " 34718: 'shivam',\n",
       " 9657: 'hurting',\n",
       " 302: 'effects',\n",
       " 52093: 'slauther',\n",
       " 52094: \"'flame'\",\n",
       " 52095: 'sommerset',\n",
       " 52096: 'interwhined',\n",
       " 27641: 'whacking',\n",
       " 52097: 'bartok',\n",
       " 8778: 'barton',\n",
       " 21912: 'frewer',\n",
       " 52098: \"fi'\",\n",
       " 6195: 'ingrid',\n",
       " 30578: 'stribor',\n",
       " 52099: 'approporiately',\n",
       " 52100: 'wobblyhand',\n",
       " 52101: 'tantalisingly',\n",
       " 52102: 'ankylosaurus',\n",
       " 17637: 'parasites',\n",
       " 52103: 'childen',\n",
       " 52104: \"jenkins'\",\n",
       " 52105: 'metafiction',\n",
       " 17638: 'golem',\n",
       " 40863: 'indiscretion',\n",
       " 23386: \"reeves'\",\n",
       " 57784: \"inamorata's\",\n",
       " 52107: 'brittannica',\n",
       " 7919: 'adapt',\n",
       " 30579: \"russo's\",\n",
       " 48249: 'guitarists',\n",
       " 10556: 'abbott',\n",
       " 40864: 'abbots',\n",
       " 17652: 'lanisha',\n",
       " 40866: 'magickal',\n",
       " 52108: 'mattter',\n",
       " 52109: \"'willy\",\n",
       " 34719: 'pumpkins',\n",
       " 52110: 'stuntpeople',\n",
       " 30580: 'estimate',\n",
       " 40867: 'ugghhh',\n",
       " 11312: 'gameplay',\n",
       " 52111: \"wern't\",\n",
       " 40868: \"n'sync\",\n",
       " 16120: 'sickeningly',\n",
       " 40869: 'chiara',\n",
       " 4014: 'disturbed',\n",
       " 40870: 'portmanteau',\n",
       " 52112: 'ineffectively',\n",
       " 82146: \"duchonvey's\",\n",
       " 37522: \"nasty'\",\n",
       " 1288: 'purpose',\n",
       " 52115: 'lazers',\n",
       " 28108: 'lightened',\n",
       " 52116: 'kaliganj',\n",
       " 52117: 'popularism',\n",
       " 18514: \"damme's\",\n",
       " 30581: 'stylistics',\n",
       " 52118: 'mindgaming',\n",
       " 46452: 'spoilerish',\n",
       " 52120: \"'corny'\",\n",
       " 34721: 'boerner',\n",
       " 6795: 'olds',\n",
       " 52121: 'bakelite',\n",
       " 27642: 'renovated',\n",
       " 27643: 'forrester',\n",
       " 52122: \"lumiere's\",\n",
       " 52027: 'gaskets',\n",
       " 887: 'needed',\n",
       " 34722: 'smight',\n",
       " 1300: 'master',\n",
       " 25908: \"edie's\",\n",
       " 40871: 'seeber',\n",
       " 52123: 'hiya',\n",
       " 52124: 'fuzziness',\n",
       " 14900: 'genesis',\n",
       " 12610: 'rewards',\n",
       " 30582: 'enthrall',\n",
       " 40872: \"'about\",\n",
       " 52125: \"recollection's\",\n",
       " 11042: 'mutilated',\n",
       " 52126: 'fatherlands',\n",
       " 52127: \"fischer's\",\n",
       " 5402: 'positively',\n",
       " 34708: '270',\n",
       " 34723: 'ahmed',\n",
       " 9839: 'zatoichi',\n",
       " 13889: 'bannister',\n",
       " 52130: 'anniversaries',\n",
       " 30583: \"helm's\",\n",
       " 52131: \"'work'\",\n",
       " 34724: 'exclaimed',\n",
       " 52132: \"'unfunny'\",\n",
       " 52032: '274',\n",
       " 547: 'feeling',\n",
       " 52134: \"wanda's\",\n",
       " 33269: 'dolan',\n",
       " 52136: '278',\n",
       " 52137: 'peacoat',\n",
       " 40873: 'brawny',\n",
       " 40874: 'mishra',\n",
       " 40875: 'worlders',\n",
       " 52138: 'protags',\n",
       " 52139: 'skullcap',\n",
       " 57599: 'dastagir',\n",
       " 5625: 'affairs',\n",
       " 7802: 'wholesome',\n",
       " 52140: 'hymen',\n",
       " 25249: 'paramedics',\n",
       " 52141: 'unpersons',\n",
       " 52142: 'heavyarms',\n",
       " 52143: 'affaire',\n",
       " 52144: 'coulisses',\n",
       " 40876: 'hymer',\n",
       " 52145: 'kremlin',\n",
       " 30584: 'shipments',\n",
       " 52146: 'pixilated',\n",
       " 30585: \"'00s\",\n",
       " 18515: 'diminishing',\n",
       " 1360: 'cinematic',\n",
       " 14901: 'resonates',\n",
       " 40877: 'simplify',\n",
       " 40878: \"nature'\",\n",
       " 40879: 'temptresses',\n",
       " 16825: 'reverence',\n",
       " 19505: 'resonated',\n",
       " 34725: 'dailey',\n",
       " 52147: '2\\x85',\n",
       " 27644: 'treize',\n",
       " 52148: 'majo',\n",
       " 21913: 'kiya',\n",
       " 52149: 'woolnough',\n",
       " 39800: 'thanatos',\n",
       " 35734: 'sandoval',\n",
       " 40882: 'dorama',\n",
       " 52150: \"o'shaughnessy\",\n",
       " 4991: 'tech',\n",
       " 32021: 'fugitives',\n",
       " 30586: 'teck',\n",
       " 76128: \"'e'\",\n",
       " 40884: 'doesn’t',\n",
       " 52152: 'purged',\n",
       " 660: 'saying',\n",
       " 41098: \"martians'\",\n",
       " 23421: 'norliss',\n",
       " 27645: 'dickey',\n",
       " 52155: 'dicker',\n",
       " 52156: \"'sependipity\",\n",
       " 8425: 'padded',\n",
       " 57795: 'ordell',\n",
       " 40885: \"sturges'\",\n",
       " 52157: 'independentcritics',\n",
       " 5748: 'tempted',\n",
       " 34727: \"atkinson's\",\n",
       " 25250: 'hounded',\n",
       " 52158: 'apace',\n",
       " 15497: 'clicked',\n",
       " 30587: \"'humor'\",\n",
       " 17180: \"martino's\",\n",
       " 52159: \"'supporting\",\n",
       " 52035: 'warmongering',\n",
       " 34728: \"zemeckis's\",\n",
       " 21914: 'lube',\n",
       " 52160: 'shocky',\n",
       " 7479: 'plate',\n",
       " 40886: 'plata',\n",
       " 40887: 'sturgess',\n",
       " 40888: \"nerds'\",\n",
       " 20603: 'plato',\n",
       " 34729: 'plath',\n",
       " 40889: 'platt',\n",
       " 52162: 'mcnab',\n",
       " 27646: 'clumsiness',\n",
       " 3902: 'altogether',\n",
       " 42587: 'massacring',\n",
       " 52163: 'bicenntinial',\n",
       " 40890: 'skaal',\n",
       " 14363: 'droning',\n",
       " 8779: 'lds',\n",
       " 21915: 'jaguar',\n",
       " 34730: \"cale's\",\n",
       " 1780: 'nicely',\n",
       " 4591: 'mummy',\n",
       " 18516: \"lot's\",\n",
       " 10089: 'patch',\n",
       " 50205: 'kerkhof',\n",
       " 52164: \"leader's\",\n",
       " 27647: \"'movie\",\n",
       " 52165: 'uncomfirmed',\n",
       " 40891: 'heirloom',\n",
       " 47363: 'wrangle',\n",
       " 52166: 'emotion\\x85',\n",
       " 52167: \"'stargate'\",\n",
       " 40892: 'pinoy',\n",
       " 40893: 'conchatta',\n",
       " 41131: 'broeke',\n",
       " 40894: 'advisedly',\n",
       " 17639: \"barker's\",\n",
       " 52169: 'descours',\n",
       " 775: 'lots',\n",
       " 9262: 'lotr',\n",
       " 9882: 'irs',\n",
       " 52170: 'lott',\n",
       " 40895: 'xvi',\n",
       " 34731: 'irk',\n",
       " 52171: 'irl',\n",
       " 6890: 'ira',\n",
       " 21916: 'belzer',\n",
       " 52172: 'irc',\n",
       " 27648: 'ire',\n",
       " 40896: 'requisites',\n",
       " 7696: 'discipline',\n",
       " 52964: 'lyoko',\n",
       " 11313: 'extend',\n",
       " 876: 'nature',\n",
       " 52173: \"'dickie'\",\n",
       " 40897: 'optimist',\n",
       " 30589: 'lapping',\n",
       " 3903: 'superficial',\n",
       " 52174: 'vestment',\n",
       " 2826: 'extent',\n",
       " 52175: 'tendons',\n",
       " 52176: \"heller's\",\n",
       " 52177: 'quagmires',\n",
       " 52178: 'miyako',\n",
       " 20604: 'moocow',\n",
       " 52179: \"coles'\",\n",
       " 40898: 'lookit',\n",
       " 52180: 'ravenously',\n",
       " 40899: 'levitating',\n",
       " 52181: 'perfunctorily',\n",
       " 30590: 'lookin',\n",
       " 40901: \"lot'\",\n",
       " 52182: 'lookie',\n",
       " 34873: 'fearlessly',\n",
       " 52184: 'libyan',\n",
       " 40902: 'fondles',\n",
       " 35717: 'gopher',\n",
       " 40904: 'wearying',\n",
       " 52185: \"nz's\",\n",
       " 27649: 'minuses',\n",
       " 52186: 'puposelessly',\n",
       " 52187: 'shandling',\n",
       " 31271: 'decapitates',\n",
       " 11932: 'humming',\n",
       " 40905: \"'nother\",\n",
       " 21917: 'smackdown',\n",
       " 30591: 'underdone',\n",
       " 40906: 'frf',\n",
       " 52188: 'triviality',\n",
       " 25251: 'fro',\n",
       " 8780: 'bothers',\n",
       " 52189: \"'kensington\",\n",
       " 76: 'much',\n",
       " 34733: 'muco',\n",
       " 22618: 'wiseguy',\n",
       " 27651: \"richie's\",\n",
       " 40907: 'tonino',\n",
       " 52190: 'unleavened',\n",
       " 11590: 'fry',\n",
       " 40908: \"'tv'\",\n",
       " 40909: 'toning',\n",
       " 14364: 'obese',\n",
       " 30592: 'sensationalized',\n",
       " 40910: 'spiv',\n",
       " 6262: 'spit',\n",
       " 7367: 'arkin',\n",
       " 21918: 'charleton',\n",
       " 16826: 'jeon',\n",
       " 21919: 'boardroom',\n",
       " 4992: 'doubts',\n",
       " 3087: 'spin',\n",
       " 53086: 'hepo',\n",
       " 27652: 'wildcat',\n",
       " 10587: 'venoms',\n",
       " 52194: 'misconstrues',\n",
       " 18517: 'mesmerising',\n",
       " 40911: 'misconstrued',\n",
       " 52195: 'rescinds',\n",
       " 52196: 'prostrate',\n",
       " 40912: 'majid',\n",
       " 16482: 'climbed',\n",
       " 34734: 'canoeing',\n",
       " 52198: 'majin',\n",
       " 57807: 'animie',\n",
       " 40913: 'sylke',\n",
       " 14902: 'conditioned',\n",
       " 40914: 'waddell',\n",
       " 52199: '3\\x85',\n",
       " 41191: 'hyperdrive',\n",
       " 34735: 'conditioner',\n",
       " 53156: 'bricklayer',\n",
       " 2579: 'hong',\n",
       " 52201: 'memoriam',\n",
       " 30595: 'inventively',\n",
       " 25252: \"levant's\",\n",
       " 20641: 'portobello',\n",
       " 52203: 'remand',\n",
       " 19507: 'mummified',\n",
       " 27653: 'honk',\n",
       " 19508: 'spews',\n",
       " 40915: 'visitations',\n",
       " 52204: 'mummifies',\n",
       " 25253: 'cavanaugh',\n",
       " 23388: 'zeon',\n",
       " 40916: \"jungle's\",\n",
       " 34736: 'viertel',\n",
       " 27654: 'frenchmen',\n",
       " 52205: 'torpedoes',\n",
       " 52206: 'schlessinger',\n",
       " 34737: 'torpedoed',\n",
       " 69879: 'blister',\n",
       " 52207: 'cinefest',\n",
       " 34738: 'furlough',\n",
       " 52208: 'mainsequence',\n",
       " 40917: 'mentors',\n",
       " 9097: 'academic',\n",
       " 20605: 'stillness',\n",
       " 40918: 'academia',\n",
       " 52209: 'lonelier',\n",
       " 52210: 'nibby',\n",
       " 52211: \"losers'\",\n",
       " 40919: 'cineastes',\n",
       " 4452: 'corporate',\n",
       " 40920: 'massaging',\n",
       " 30596: 'bellow',\n",
       " 19509: 'absurdities',\n",
       " 53244: 'expetations',\n",
       " 40921: 'nyfiken',\n",
       " 75641: 'mehras',\n",
       " 52212: 'lasse',\n",
       " 52213: 'visability',\n",
       " 33949: 'militarily',\n",
       " 52214: \"elder'\",\n",
       " 19026: 'gainsbourg',\n",
       " 20606: 'hah',\n",
       " 13423: 'hai',\n",
       " 34739: 'haj',\n",
       " 25254: 'hak',\n",
       " 4314: 'hal',\n",
       " 4895: 'ham',\n",
       " 53262: 'duffer',\n",
       " 52216: 'haa',\n",
       " 69: 'had',\n",
       " 11933: 'advancement',\n",
       " 16828: 'hag',\n",
       " 25255: \"hand'\",\n",
       " 13424: 'hay',\n",
       " 20607: 'mcnamara',\n",
       " 52217: \"mozart's\",\n",
       " 30734: 'duffel',\n",
       " 30597: 'haq',\n",
       " 13890: 'har',\n",
       " 47: 'has',\n",
       " 2404: 'hat',\n",
       " 40922: 'hav',\n",
       " 30598: 'haw',\n",
       " 52218: 'figtings',\n",
       " 15498: 'elders',\n",
       " 52219: 'underpanted',\n",
       " 52220: 'pninson',\n",
       " 27655: 'unequivocally',\n",
       " 23676: \"barbara's\",\n",
       " 52222: \"bello'\",\n",
       " 13000: 'indicative',\n",
       " 40923: 'yawnfest',\n",
       " 52223: 'hexploitation',\n",
       " 52224: \"loder's\",\n",
       " 27656: 'sleuthing',\n",
       " 32625: \"justin's\",\n",
       " 52225: \"'ball\",\n",
       " 52226: \"'summer\",\n",
       " 34938: \"'demons'\",\n",
       " 52228: \"mormon's\",\n",
       " 34740: \"laughton's\",\n",
       " 52229: 'debell',\n",
       " 39727: 'shipyard',\n",
       " 30600: 'unabashedly',\n",
       " 40404: 'disks',\n",
       " 2293: 'crowd',\n",
       " 10090: 'crowe',\n",
       " 56437: \"vancouver's\",\n",
       " 34741: 'mosques',\n",
       " 6630: 'crown',\n",
       " 52230: 'culpas',\n",
       " 27657: 'crows',\n",
       " 53347: 'surrell',\n",
       " 52232: 'flowless',\n",
       " 52233: 'sheirk',\n",
       " 40926: \"'three\",\n",
       " 52234: \"peterson'\",\n",
       " 52235: 'ooverall',\n",
       " 40927: 'perchance',\n",
       " 1324: 'bottom',\n",
       " 53366: 'chabert',\n",
       " 52236: 'sneha',\n",
       " 13891: 'inhuman',\n",
       " 52237: 'ichii',\n",
       " 52238: 'ursla',\n",
       " 30601: 'completly',\n",
       " 40928: 'moviedom',\n",
       " 52239: 'raddick',\n",
       " 51998: 'brundage',\n",
       " 40929: 'brigades',\n",
       " 1184: 'starring',\n",
       " 52240: \"'goal'\",\n",
       " 52241: 'caskets',\n",
       " 52242: 'willcock',\n",
       " 52243: \"threesome's\",\n",
       " 52244: \"mosque'\",\n",
       " 52245: \"cover's\",\n",
       " 17640: 'spaceships',\n",
       " 40930: 'anomalous',\n",
       " 27658: 'ptsd',\n",
       " 52246: 'shirdan',\n",
       " 21965: 'obscenity',\n",
       " 30602: 'lemmings',\n",
       " 30603: 'duccio',\n",
       " 52247: \"levene's\",\n",
       " 52248: \"'gorby'\",\n",
       " 25258: \"teenager's\",\n",
       " 5343: 'marshall',\n",
       " 9098: 'honeymoon',\n",
       " 3234: 'shoots',\n",
       " 12261: 'despised',\n",
       " 52249: 'okabasho',\n",
       " 8292: 'fabric',\n",
       " 18518: 'cannavale',\n",
       " 3540: 'raped',\n",
       " 52250: \"tutt's\",\n",
       " 17641: 'grasping',\n",
       " 18519: 'despises',\n",
       " 40931: \"thief's\",\n",
       " 8929: 'rapes',\n",
       " 52251: 'raper',\n",
       " 27659: \"eyre'\",\n",
       " 52252: 'walchek',\n",
       " 23389: \"elmo's\",\n",
       " 40932: 'perfumes',\n",
       " 21921: 'spurting',\n",
       " 52253: \"exposition'\\x85\",\n",
       " 52254: 'denoting',\n",
       " 34743: 'thesaurus',\n",
       " 40933: \"shoot'\",\n",
       " 49762: 'bonejack',\n",
       " 52256: 'simpsonian',\n",
       " 30604: 'hebetude',\n",
       " 34744: \"hallow's\",\n",
       " 52257: 'desperation\\x85',\n",
       " 34745: 'incinerator',\n",
       " 10311: 'congratulations',\n",
       " 52258: 'humbled',\n",
       " 5927: \"else's\",\n",
       " 40848: 'trelkovski',\n",
       " 52259: \"rape'\",\n",
       " 59389: \"'chapters'\",\n",
       " 52260: '1600s',\n",
       " 7256: 'martian',\n",
       " 25259: 'nicest',\n",
       " 52262: 'eyred',\n",
       " 9460: 'passenger',\n",
       " 6044: 'disgrace',\n",
       " 52263: 'moderne',\n",
       " 5123: 'barrymore',\n",
       " 52264: 'yankovich',\n",
       " 40934: 'moderns',\n",
       " 52265: 'studliest',\n",
       " 52266: 'bedsheet',\n",
       " 14903: 'decapitation',\n",
       " 52267: 'slurring',\n",
       " 52268: \"'nunsploitation'\",\n",
       " 34746: \"'character'\",\n",
       " 9883: 'cambodia',\n",
       " 52269: 'rebelious',\n",
       " 27660: 'pasadena',\n",
       " 40935: 'crowne',\n",
       " 52270: \"'bedchamber\",\n",
       " 52271: 'conjectural',\n",
       " 52272: 'appologize',\n",
       " 52273: 'halfassing',\n",
       " 57819: 'paycheque',\n",
       " 20609: 'palms',\n",
       " 52274: \"'islands\",\n",
       " 40936: 'hawked',\n",
       " 21922: 'palme',\n",
       " 40937: 'conservatively',\n",
       " 64010: 'larp',\n",
       " 5561: 'palma',\n",
       " 21923: 'smelling',\n",
       " 13001: 'aragorn',\n",
       " 52275: 'hawker',\n",
       " 52276: 'hawkes',\n",
       " 3978: 'explosions',\n",
       " 8062: 'loren',\n",
       " 52277: \"pyle's\",\n",
       " 6707: 'shootout',\n",
       " 18520: \"mike's\",\n",
       " 52278: \"driscoll's\",\n",
       " 40938: 'cogsworth',\n",
       " 52279: \"britian's\",\n",
       " 34747: 'childs',\n",
       " 52280: \"portrait's\",\n",
       " 3629: 'chain',\n",
       " 2500: 'whoever',\n",
       " 52281: 'puttered',\n",
       " 52282: 'childe',\n",
       " 52283: 'maywether',\n",
       " 3039: 'chair',\n",
       " 52284: \"rance's\",\n",
       " 34748: 'machu',\n",
       " 4520: 'ballet',\n",
       " 34749: 'grapples',\n",
       " 76155: 'summerize',\n",
       " 30606: 'freelance',\n",
       " 52286: \"andrea's\",\n",
       " 52287: '\\x91very',\n",
       " 45882: 'coolidge',\n",
       " 18521: 'mache',\n",
       " 52288: 'balled',\n",
       " 40940: 'grappled',\n",
       " 18522: 'macha',\n",
       " 21924: 'underlining',\n",
       " 5626: 'macho',\n",
       " 19510: 'oversight',\n",
       " 25260: 'machi',\n",
       " 11314: 'verbally',\n",
       " 21925: 'tenacious',\n",
       " 40941: 'windshields',\n",
       " 18560: 'paychecks',\n",
       " 3399: 'jerk',\n",
       " 11934: \"good'\",\n",
       " 34751: 'prancer',\n",
       " 21926: 'prances',\n",
       " 52289: 'olympus',\n",
       " 21927: 'lark',\n",
       " 10788: 'embark',\n",
       " 7368: 'gloomy',\n",
       " 52290: 'jehaan',\n",
       " 52291: 'turaqui',\n",
       " 20610: \"child'\",\n",
       " 2897: 'locked',\n",
       " 52292: 'pranced',\n",
       " 2591: 'exact',\n",
       " 52293: 'unattuned',\n",
       " 786: 'minute',\n",
       " 16121: 'skewed',\n",
       " 40943: 'hodgins',\n",
       " 34752: 'skewer',\n",
       " 52294: 'think\\x85',\n",
       " 38768: 'rosenstein',\n",
       " 52295: 'helmit',\n",
       " 34753: 'wrestlemanias',\n",
       " 16829: 'hindered',\n",
       " 30607: \"martha's\",\n",
       " 52296: 'cheree',\n",
       " 52297: \"pluckin'\",\n",
       " 40944: 'ogles',\n",
       " 11935: 'heavyweight',\n",
       " 82193: 'aada',\n",
       " 11315: 'chopping',\n",
       " 61537: 'strongboy',\n",
       " 41345: 'hegemonic',\n",
       " 40945: 'adorns',\n",
       " 41349: 'xxth',\n",
       " 34754: 'nobuhiro',\n",
       " 52301: 'capitães',\n",
       " 52302: 'kavogianni',\n",
       " 13425: 'antwerp',\n",
       " 6541: 'celebrated',\n",
       " 52303: 'roarke',\n",
       " 40946: 'baggins',\n",
       " 31273: 'cheeseburgers',\n",
       " 52304: 'matras',\n",
       " 52305: \"nineties'\",\n",
       " 52306: \"'craig'\",\n",
       " 13002: 'celebrates',\n",
       " 3386: 'unintentionally',\n",
       " 14365: 'drafted',\n",
       " 52307: 'climby',\n",
       " 52308: '303',\n",
       " 18523: 'oldies',\n",
       " 9099: 'climbs',\n",
       " 9658: 'honour',\n",
       " 34755: 'plucking',\n",
       " 30077: '305',\n",
       " 5517: 'address',\n",
       " 40947: 'menjou',\n",
       " 42595: \"'freak'\",\n",
       " 19511: 'dwindling',\n",
       " 9461: 'benson',\n",
       " 52310: 'white’s',\n",
       " 40948: 'shamelessness',\n",
       " 21928: 'impacted',\n",
       " 52311: 'upatz',\n",
       " 3843: 'cusack',\n",
       " 37570: \"flavia's\",\n",
       " 52312: 'effette',\n",
       " 34756: 'influx',\n",
       " 52313: 'boooooooo',\n",
       " 52314: 'dimitrova',\n",
       " 13426: 'houseman',\n",
       " 25262: 'bigas',\n",
       " 52315: 'boylen',\n",
       " 52316: 'phillipenes',\n",
       " 40949: 'fakery',\n",
       " 27661: \"grandpa's\",\n",
       " 27662: 'darnell',\n",
       " 19512: 'undergone',\n",
       " 52318: 'handbags',\n",
       " 21929: 'perished',\n",
       " 37781: 'pooped',\n",
       " 27663: 'vigour',\n",
       " 3630: 'opposed',\n",
       " 52319: 'etude',\n",
       " 11802: \"caine's\",\n",
       " 52320: 'doozers',\n",
       " 34757: 'photojournals',\n",
       " 52321: 'perishes',\n",
       " 34758: 'constrains',\n",
       " 40951: 'migenes',\n",
       " 30608: 'consoled',\n",
       " 16830: 'alastair',\n",
       " 52322: 'wvs',\n",
       " 52323: 'ooooooh',\n",
       " 34759: 'approving',\n",
       " 40952: 'consoles',\n",
       " 52067: 'disparagement',\n",
       " 52325: 'futureistic',\n",
       " 52326: 'rebounding',\n",
       " 52327: \"'date\",\n",
       " 52328: 'gregoire',\n",
       " 21930: 'rutherford',\n",
       " 34760: 'americanised',\n",
       " 82199: 'novikov',\n",
       " 1045: 'following',\n",
       " 34761: 'munroe',\n",
       " 52329: \"morita'\",\n",
       " 52330: 'christenssen',\n",
       " 23109: 'oatmeal',\n",
       " 25263: 'fossey',\n",
       " 40953: 'livered',\n",
       " 13003: 'listens',\n",
       " 76167: \"'marci\",\n",
       " 52333: \"otis's\",\n",
       " 23390: 'thanking',\n",
       " 16022: 'maude',\n",
       " 34762: 'extensions',\n",
       " 52335: 'ameteurish',\n",
       " 52336: \"commender's\",\n",
       " 27664: 'agricultural',\n",
       " 4521: 'convincingly',\n",
       " 17642: 'fueled',\n",
       " 54017: 'mahattan',\n",
       " 40955: \"paris's\",\n",
       " 52339: 'vulkan',\n",
       " 52340: 'stapes',\n",
       " 52341: 'odysessy',\n",
       " 12262: 'harmon',\n",
       " 4255: 'surfing',\n",
       " 23497: 'halloran',\n",
       " 49583: 'unbelieveably',\n",
       " 52342: \"'offed'\",\n",
       " 30610: 'quadrant',\n",
       " 19513: 'inhabiting',\n",
       " 34763: 'nebbish',\n",
       " 40956: 'forebears',\n",
       " 34764: 'skirmish',\n",
       " 52343: 'ocassionally',\n",
       " 52344: \"'resist\",\n",
       " 21931: 'impactful',\n",
       " 52345: 'spicier',\n",
       " 40957: 'touristy',\n",
       " 52346: \"'football'\",\n",
       " 40958: 'webpage',\n",
       " 52348: 'exurbia',\n",
       " 52349: 'jucier',\n",
       " 14904: 'professors',\n",
       " 34765: 'structuring',\n",
       " 30611: 'jig',\n",
       " 40959: 'overlord',\n",
       " 25264: 'disconnect',\n",
       " 82204: 'sniffle',\n",
       " 40960: 'slimeball',\n",
       " 40961: 'jia',\n",
       " 16831: 'milked',\n",
       " 40962: 'banjoes',\n",
       " 1240: 'jim',\n",
       " 52351: 'workforces',\n",
       " 52352: 'jip',\n",
       " 52353: 'rotweiller',\n",
       " 34766: 'mundaneness',\n",
       " 52354: \"'ninja'\",\n",
       " 11043: \"dead'\",\n",
       " 40963: \"cipriani's\",\n",
       " 20611: 'modestly',\n",
       " 52355: \"professor'\",\n",
       " 40964: 'shacked',\n",
       " 34767: 'bashful',\n",
       " 23391: 'sorter',\n",
       " 16123: 'overpowering',\n",
       " 18524: 'workmanlike',\n",
       " 27665: 'henpecked',\n",
       " 18525: 'sorted',\n",
       " 52357: \"jōb's\",\n",
       " 52358: \"'always\",\n",
       " 34768: \"'baptists\",\n",
       " 52359: 'dreamcatchers',\n",
       " 52360: \"'silence'\",\n",
       " 21932: 'hickory',\n",
       " 52361: 'fun\\x97yet',\n",
       " 52362: 'breakumentary',\n",
       " 15499: 'didn',\n",
       " 52363: 'didi',\n",
       " 52364: 'pealing',\n",
       " 40965: 'dispite',\n",
       " 25265: \"italy's\",\n",
       " 21933: 'instability',\n",
       " 6542: 'quarter',\n",
       " 12611: 'quartet',\n",
       " 52365: 'padmé',\n",
       " 52366: \"'bleedmedry\",\n",
       " 52367: 'pahalniuk',\n",
       " 52368: 'honduras',\n",
       " 10789: 'bursting',\n",
       " 41468: \"pablo's\",\n",
       " 52370: 'irremediably',\n",
       " 40966: 'presages',\n",
       " 57835: 'bowlegged',\n",
       " 65186: 'dalip',\n",
       " 6263: 'entering',\n",
       " 76175: 'newsradio',\n",
       " 54153: 'presaged',\n",
       " 27666: \"giallo's\",\n",
       " 40967: 'bouyant',\n",
       " 52371: 'amerterish',\n",
       " 18526: 'rajni',\n",
       " 30613: 'leeves',\n",
       " 34770: 'macauley',\n",
       " 615: 'seriously',\n",
       " 52372: 'sugercoma',\n",
       " 52373: 'grimstead',\n",
       " 52374: \"'fairy'\",\n",
       " 30614: 'zenda',\n",
       " 52375: \"'twins'\",\n",
       " 17643: 'realisation',\n",
       " 27667: 'highsmith',\n",
       " 7820: 'raunchy',\n",
       " 40968: 'incentives',\n",
       " 52377: 'flatson',\n",
       " 35100: 'snooker',\n",
       " 16832: 'crazies',\n",
       " 14905: 'crazier',\n",
       " 7097: 'grandma',\n",
       " 52378: 'napunsaktha',\n",
       " 30615: 'workmanship',\n",
       " 52379: 'reisner',\n",
       " 61309: \"sanford's\",\n",
       " 52380: '\\x91doña',\n",
       " 6111: 'modest',\n",
       " 19156: \"everything's\",\n",
       " 40969: 'hamer',\n",
       " 52382: \"couldn't'\",\n",
       " 13004: 'quibble',\n",
       " 52383: 'socking',\n",
       " 21934: 'tingler',\n",
       " 52384: 'gutman',\n",
       " 40970: 'lachlan',\n",
       " 52385: 'tableaus',\n",
       " 52386: 'headbanger',\n",
       " 2850: 'spoken',\n",
       " 34771: 'cerebrally',\n",
       " 23493: \"'road\",\n",
       " 21935: 'tableaux',\n",
       " 40971: \"proust's\",\n",
       " 40972: 'periodical',\n",
       " 52388: \"shoveller's\",\n",
       " 25266: 'tamara',\n",
       " 17644: 'affords',\n",
       " 3252: 'concert',\n",
       " 87958: \"yara's\",\n",
       " 52389: 'someome',\n",
       " 8427: 'lingering',\n",
       " 41514: \"abraham's\",\n",
       " 34772: 'beesley',\n",
       " 34773: 'cherbourg',\n",
       " 28627: 'kagan',\n",
       " 9100: 'snatch',\n",
       " 9263: \"miyazaki's\",\n",
       " 25267: 'absorbs',\n",
       " 40973: \"koltai's\",\n",
       " 64030: 'tingled',\n",
       " 19514: 'crossroads',\n",
       " 16124: 'rehab',\n",
       " 52392: 'falworth',\n",
       " 52393: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word = {k+3 : v for k, v in idx_to_word.items()}\n",
    "idx_to_word[0] = '<PAD>'\n",
    "idx_to_word[1] = '<START>'\n",
    "idx_to_word[2] = '<UNK>' # unknown\n",
    "idx_to_word[3] = '<UNUSED>'\n",
    "idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화평의 길이\n",
      "영화평 0 : 218\n",
      "영화평 1 : 189\n",
      "영화평 2 : 141\n",
      "영화평 3 : 550\n",
      "영화평 4 : 147\n",
      "영화평 5 : 43\n",
      "영화평 6 : 123\n",
      "영화평 7 : 562\n",
      "영화평 8 : 233\n",
      "영화평 9 : 130\n"
     ]
    }
   ],
   "source": [
    "def show_length():\n",
    "    print('영화평의 길이')\n",
    "    for i in range(10):\n",
    "        print('영화평', i, ':', len(X_train[i]))\n",
    "\n",
    "show_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2차원 데이터로 변환하기 위해\n",
    "# 모든 영화평 길이 같게 해야 함.\n",
    "X_train = pad_sequences(X_train, maxlen=MY_LENGTH, truncating='post', padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=MY_LENGTH, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화평의 길이\n",
      "영화평 0 : 80\n",
      "영화평 1 : 80\n",
      "영화평 2 : 80\n",
      "영화평 3 : 80\n",
      "영화평 4 : 80\n",
      "영화평 5 : 80\n",
      "영화평 6 : 80\n",
      "영화평 7 : 80\n",
      "영화평 8 : 80\n",
      "영화평 9 : 80\n"
     ]
    }
   ],
   "source": [
    "show_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50, 30))\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50376"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 80, 32)            320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 344,897\n",
      "Trainable params: 344,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 구조\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=MY_WORDS, output_dim=MY_EMBED, input_length=MY_LENGTH))\n",
    "model.add(LSTM(MY_HIDDEN, input_shape=(MY_LENGTH, MY_EMBED)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics='acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 12s 77ms/step - loss: 0.5710 - acc: 0.6810\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 11s 92ms/step - loss: 0.3520 - acc: 0.8510\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 15s 120ms/step - loss: 0.2826 - acc: 0.8872\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 14s 109ms/step - loss: 0.2507 - acc: 0.9058\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 16s 130ms/step - loss: 0.2231 - acc: 0.9182\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 17s 132ms/step - loss: 0.2046 - acc: 0.9250\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 18s 143ms/step - loss: 0.1751 - acc: 0.9379\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 19s 149ms/step - loss: 0.1572 - acc: 0.9456\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.1397 - acc: 0.9508\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.1149 - acc: 0.9622\n",
      "training time: 159.16203832626343\n"
     ]
    }
   ],
   "source": [
    "begin = time()\n",
    "model.fit(X_train, Y_train, epochs=MY_EPOCHS, batch_size=MY_BATCH)\n",
    "end = time()\n",
    "\n",
    "print(f'training time: {end-begin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 12ms/step - loss: 0.7760 - acc: 0.7689\n",
      "최종 정확도: 0.77\n"
     ]
    }
   ],
   "source": [
    "# 평가용 데이터를 이용한 평가\n",
    "score = model.evaluate(x=X_test, y=Y_test, verbose=1)\n",
    "print(f'최종 정확도: {score[1]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03156692]\n",
      " [0.99798155]\n",
      " [0.98707974]\n",
      " ...\n",
      " [0.02434686]\n",
      " [0.69162285]\n",
      " [0.9899094 ]]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 이용한 긍정/부정 예측\n",
    "pred = model.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[9786 2714]\n",
      " [3063 9437]]\n"
     ]
    }
   ],
   "source": [
    "pred = (pred > 0.5)\n",
    "print('confusion matrix')\n",
    "print(confusion_matrix(Y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 텍스트를 이용한 긍정/부정 판별\n",
    "text = \"My God the actors who potrayed the VIP people cannot\\\n",
    "        act. I cringed everytime they said a line. It felt like they\\\n",
    "        were just reading them. Even the intonation was off. It was\\\n",
    "        like when we were kids and had to read a play in class and we\\\n",
    "        exagerated the intonation. Terrible, just awful.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'god', 'the', 'actors', 'who', 'potrayed', 'the', 'vip', 'people', 'cannot', 'act.', 'i', 'cringed', 'everytime', 'they', 'said', 'a', 'line.', 'it', 'felt', 'like', 'they', 'were', 'just', 'reading', 'them.', 'even', 'the', 'intonation', 'was', 'off.', 'it', 'was', 'like', 'when', 'we', 'were', 'kids', 'and', 'had', 'to', 'read', 'a', 'play', 'in', 'class', 'and', 'we', 'exagerated', 'the', 'intonation.', 'terrible,', 'just', 'awful.']\n"
     ]
    }
   ],
   "source": [
    "input_text = text.lower().split()\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = imdb.get_word_index()\n",
    "word_to_idx = {k:(v+3) for k,v in word_to_idx.items()}\n",
    "word_to_idx[\"<PAD>\"] = 0\n",
    "word_to_idx[\"<START>\"] = 1\n",
    "word_to_idx[\"<UNK>\"] = 2 # unknown\n",
    "word_to_idx[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 558, 4, 156, 37, 2, 4, 3, 84, 566, 2, 13, 3, 3, 36, 301, 6, 2, 12, 421, 40, 36, 71, 43, 886, 2, 60, 4, 3, 16, 2, 12, 16, 40, 54, 75, 71, 362, 5, 69, 8, 332, 6, 297, 11, 707, 5, 75, 3, 4, 2, 2, 43, 2]\n"
     ]
    }
   ],
   "source": [
    "# 단어 숫자로 변환\n",
    "def encoding(review_text):\n",
    "    encoded = []\n",
    "    for word in review_text:\n",
    "        try:\n",
    "            idx = word_to_idx[word]\n",
    "            if idx > 10000:\n",
    "                encoded.append(3)\n",
    "            else:\n",
    "                encoded.append(idx)\n",
    "        except:\n",
    "            encoded.append(2)\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "input_encoded = encoding(input_text)\n",
    "print(input_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1096836]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pad = pad_sequences(np.array(input_encoded)[np.newaxis, :], maxlen=MY_LENGTH, truncating='post', padding='post')\n",
    "model.predict(input_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3절. 로이터 기사 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.datasets import reuters\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_SAMPLE = 2947\n",
    "NUM_CLASS = 46 # 로이터 기사 데이터의 총 카테고리 수\n",
    "MY_NUM_WORDS = 2000 # 사전 안의 단어의 수\n",
    "MY_EPOCH = 10 # 학습횟수\n",
    "MY_BATCH = 64 # 매번 가져와 처리할 데이터의 수\n",
    "MY_HIDDEN = 512 # 은닉층의 뉴런 수\n",
    "MY_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
    "'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
    "'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
    "'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
    "'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n",
      "2121728/2110848 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = reuters.load_data(num_words=MY_NUM_WORDS, test_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== DB SHAPE INFO ==\n",
      "X_train shape =  (7859,)\n",
      "X_test shape =  (3369,)\n",
      "Y_train shape =  (7859,)\n",
      "Y_test shape =  (3369,)\n"
     ]
    }
   ],
   "source": [
    "def show_shape():\n",
    "    print('\\n== DB SHAPE INFO ==')\n",
    "    print('X_train shape = ', X_train.shape)\n",
    "    print('X_test shape = ', X_test.shape)\n",
    "    print('Y_train shape = ', Y_train.shape)\n",
    "    print('Y_test shape = ', Y_test.shape)\n",
    "    \n",
    "show_shape() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cocoa = 50\n",
      "1 grain = 378\n",
      "2 veg-oil = 66\n",
      "3 earn = 2769\n",
      "4 acq = 1701\n",
      "5 wheat = 14\n",
      "6 copper = 39\n",
      "7 housing = 15\n",
      "8 money-supply = 126\n",
      "9 coffee = 93\n",
      "10 sugar = 114\n",
      "11 trade = 337\n",
      "12 reserves = 40\n",
      "13 ship = 149\n",
      "14 cotton = 18\n",
      "15 carcass = 19\n",
      "16 crude = 387\n",
      "17 nat-gas = 33\n",
      "18 cpi = 59\n",
      "19 money-fx = 475\n",
      "20 interest = 238\n",
      "21 gnp = 91\n",
      "22 meal-feed = 10\n",
      "23 alum = 36\n",
      "24 oilseed = 56\n",
      "25 gold = 77\n",
      "26 tin = 18\n",
      "27 strategic-metal = 13\n",
      "28 livestock = 43\n",
      "29 retail = 19\n",
      "30 ipi = 38\n",
      "31 iron-steel = 34\n",
      "32 rubber = 30\n",
      "33 heat = 9\n",
      "34 jobs = 43\n",
      "35 lei = 10\n",
      "36 bop = 46\n",
      "37 zinc = 17\n",
      "38 orange = 16\n",
      "39 pet-chem = 20\n",
      "40 dlr = 32\n",
      "41 gas = 28\n",
      "42 silver = 10\n",
      "43 wpi = 19\n",
      "44 hog = 10\n",
      "45 lead = 14\n"
     ]
    }
   ],
   "source": [
    "# 레이블 별 데이터 개수\n",
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "for i in range(len(unique)):\n",
    "    print(unique[i], labels[i], '=', counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습용 데이터와 평가용 데이터의 클래스별 개수 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(Y_train, bins='auto')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.title('Train data')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(Y_test, bins='auto')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.title('Test data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article # 2947\n",
      "category 4 acq\n",
      "number of words 61\n",
      "[1, 2, 1229, 81, 8, 16, 515, 25, 270, 5, 4, 2, 1229, 111, 267, 7, 73, 2, 2, 7, 108, 13, 80, 1448, 28, 365, 12, 11, 15, 1986, 2, 69, 158, 18, 1296, 1275, 7, 2, 1627, 2, 2, 4, 393, 374, 1229, 323, 5, 2, 1229, 7, 2, 9, 25, 2, 473, 936, 4, 49, 8, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "# 샘플기사 (2947번) 카테고리와 단어의 수, 실제 데이터 출력\n",
    "print('article #', MY_SAMPLE)\n",
    "print('category', Y_train[MY_SAMPLE], labels[Y_train[MY_SAMPLE]])\n",
    "print('number of words', len(X_train[MY_SAMPLE]))\n",
    "print(X_train[MY_SAMPLE]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 0us/step\n",
      "565248/550378 [==============================] - 0s 0us/step\n",
      "딕셔너리에 30980개 단어 존재함.\n",
      "단어 the 의 인덱스: 1\n"
     ]
    }
   ],
   "source": [
    "# 단어-인덱스 정보 불러옴\n",
    "word_to_idx = reuters.get_word_index()\n",
    "\n",
    "print(f'딕셔너리에 {len(word_to_idx) + 1}개 단어 존재함.')\n",
    "print(f'단어 the 의 인덱스: {word_to_idx[\"the\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어-인덱스 => 인덱스-단어 딕셔너리 변환\n",
    "idx_to_word = dict([(val, key) for (key, val) in idx_to_word.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['???', '???', 'south', 'bad', 'and', 'i', 'genre', 'film', 'looking', '<UNK>', '<START>', '???', 'south', 'films', 'believe', 'the', 'can', '???', '???', 'the', 'characters', 'br', 'been', 'bed', 'you', 'kids', 'is', 'to', 'it', 'm', '???', 'really', 'thing', 'that', 'beautifully', 'army', 'the', '???', 'professional', '???', '???', '<START>', 'episode', 'truly', 'south', 'half', '<UNK>', '???', 'south', 'the', '???', 'a', 'film', '???', '4', 'deep', '<START>', 'out', 'and', 'this', 'is']\n"
     ]
    }
   ],
   "source": [
    "# 인덱스를 문자로 디코딩\n",
    "def decoding():\n",
    "    decoded = []\n",
    "    for i in X_train[MY_SAMPLE]:\n",
    "        word = idx_to_word.get(i-3, '???')\n",
    "        decoded.append(word)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "print(decoding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words=MY_NUM_WORDS)\n",
    "X_train_tok = tok.sequences_to_matrix(X_train, mode='count')\n",
    "X_test_tok = tok.sequences_to_matrix(X_test, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array size:  2000\n",
      "sum:  61.0\n"
     ]
    }
   ],
   "source": [
    "sample = X_train_tok[MY_SAMPLE]\n",
    "# print(*sample, sep=' ')\n",
    "print('array size: ', len(sample))\n",
    "print('sum: ', np.sum(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 512)               1024512   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 46)                23598     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 46)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,048,110\n",
      "Trainable params: 1,048,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 단순 은닉층 갖는 DNN\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(MY_HIDDEN, input_shape=(MY_NUM_WORDS,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(MY_DROPOUT))\n",
    "\n",
    "model.add(Dense(NUM_CLASS))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "123/123 [==============================] - 2s 9ms/step - loss: 1.6425 - accuracy: 0.6696 - val_loss: 1.0820 - val_accuracy: 0.7744\n",
      "Epoch 2/10\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.8293 - accuracy: 0.8198 - val_loss: 0.9364 - val_accuracy: 0.7925\n",
      "Epoch 3/10\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.5739 - accuracy: 0.8708 - val_loss: 0.8772 - val_accuracy: 0.8145\n",
      "Epoch 4/10\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.4301 - accuracy: 0.8987 - val_loss: 0.8780 - val_accuracy: 0.8100\n",
      "Epoch 5/10\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.3359 - accuracy: 0.9206 - val_loss: 0.8992 - val_accuracy: 0.8175\n",
      "Epoch 6/10\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2854 - accuracy: 0.9317 - val_loss: 0.9105 - val_accuracy: 0.8127\n",
      "Epoch 7/10\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2397 - accuracy: 0.9406 - val_loss: 0.9393 - val_accuracy: 0.8100\n",
      "Epoch 8/10\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2227 - accuracy: 0.9452 - val_loss: 0.9543 - val_accuracy: 0.8085\n",
      "Epoch 9/10\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 0.2200 - accuracy: 0.9448 - val_loss: 1.0248 - val_accuracy: 0.8074\n",
      "Epoch 10/10\n",
      "123/123 [==============================] - 1s 7ms/step - loss: 0.2116 - accuracy: 0.9448 - val_loss: 0.9862 - val_accuracy: 0.8121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x201a045efa0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tok, Y_train, validation_data=(X_test_tok, Y_test), epochs=MY_EPOCH, batch_size=MY_BATCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 기사의 예측값: 4 acq\n",
      "실제 정답: 4 acq\n"
     ]
    }
   ],
   "source": [
    "sample = X_train_tok[MY_SAMPLE]\n",
    "sample = sample.reshape(1, sample.shape[0])\n",
    "\n",
    "pred = model.predict(sample, verbose=0)\n",
    "guess = np.argmax(pred)\n",
    "answer = Y_train[MY_SAMPLE]\n",
    "\n",
    "print(\"샘플 기사의 예측값:\", guess, labels[guess])\n",
    "print(\"실제 정답:\", answer, labels[answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 0s 3ms/step - loss: 0.9862 - accuracy: 0.8121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.986157238483429, 0.812110424041748]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_tok, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameli\\anaconda3\\envs\\saltlux_deep_lecture\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(512,), max_iter=10)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(MY_HIDDEN,), solver=\"adam\", activation=\"relu\", max_iter=10)\n",
    "mlp.fit(X_train_tok, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796675571386168"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test_tok, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4절. 영-한 번역기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import randint\n",
    "from time import time\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_HIDDEN = 128 # 은닉층의 뉴런의 수\n",
    "MY_EPOCHS = 500 # 학습횟수 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영문자와 한글 문자를 연결한 알파벳/문자 데이터 생성\n",
    "arr1 = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "arr2 = pd.read_csv('datasets/korean.csv', header=None)\n",
    "arr2 = arr2[0].values.tolist()\n",
    "num_to_char = arr1 + arr2\n",
    "len(num_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글자 사전\n",
      " {'S': 0, 'E': 1, 'P': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, '가': 29, '각': 30, '간': 31, '감': 32, '개': 33, '거': 34, '것': 35, '게': 36, '계': 37, '고': 38, '관': 39, '광': 40, '구': 41, '굴': 42, '규': 43, '그': 44, '금': 45, '기': 46, '깊': 47, '나': 48, '날': 49, '남': 50, '내': 51, '넓': 52, '녀': 53, '노': 54, '놀': 55, '농': 56, '높': 57, '뉴': 58, '늦': 59, '다': 60, '단': 61, '도': 62, '동': 63, '들': 64, '람': 65, '랑': 66, '래': 67, '램': 68, '류': 69, '름': 70, '릎': 71, '리': 72, '많': 73, '망': 74, '매': 75, '머': 76, '먼': 77, '멍': 78, '메': 79, '명': 80, '모': 81, '목': 82, '무': 83, '물': 84, '미': 85, '바': 86, '반': 87, '방': 88, '번': 89, '복': 90, '부': 91, '분': 92, '붕': 93, '비': 94, '뿌': 95, '사': 96, '상': 97, '색': 98, '생': 99, '서': 100, '선': 101, '소': 102, '손': 103, '수': 104, '쉽': 105, '스': 106, '시': 107, '식': 108, '실': 109, '싸': 110, '아': 111, '약': 112, '얇': 113, '어': 114, '언': 115, '얼': 116, '여': 117, '연': 118, '오': 119, '옥': 120, '왼': 121, '요': 122, '용': 123, '우': 124, '운': 125, '움': 126, '위': 127, '유': 128, '은': 129, '을': 130, '음': 131, '의': 132, '이': 133, '익': 134, '인': 135, '읽': 136, '입': 137, '자': 138, '작': 139, '장': 140, '적': 141, '제': 142, '좋': 143, '주': 144, '지': 145, '짜': 146, '쪽': 147, '찾': 148, '책': 149, '출': 150, '칙': 151, '크': 152, '키': 153, '탈': 154, '택': 155, '통': 156, '파': 157, '팔': 158, '편': 159, '피': 160, '핑': 161, '한': 162, '합': 163, '해': 164, '행': 165, '험': 166, '회': 167, '획': 168, '휴': 169, '흐': 170}\n",
      "총 글자 수: 171\n",
      "영어 글자 수: 29\n",
      "한글 글자 수: 142\n"
     ]
    }
   ],
   "source": [
    "# 각 알파벳/문자에 엔덱스 부여\n",
    "char_to_num = {n : i for i, n in enumerate(num_to_char)}\n",
    "n_input = len(char_to_num)\n",
    "\n",
    "print('글자 사전\\n', char_to_num)\n",
    "print(f'총 글자 수: {len(char_to_num)}')\n",
    "print(f'영어 글자 수: {len(arr1)}')\n",
    "print(f'한글 글자 수: {len(arr2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전 내용\n",
      " [['cold', '감기'], ['come', '오다'], ['cook', '요리'], ['copy', '복사'], ['cost', '비용'], ['date', '날짜'], ['deal', '거래'], ['deep', '깊은'], ['desk', '책상'], ['down', '아래'], ['dust', '먼지'], ['duty', '의무'], ['each', '각각'], ['east', '동쪽'], ['easy', '쉽다'], ['exit', '탈출'], ['face', '얼굴'], ['fact', '사실'], ['fall', '가을'], ['farm', '농장'], ['feet', '다리'], ['find', '찾다'], ['fine', '좋다'], ['fish', '생선'], ['flow', '흐름'], ['fund', '기금'], ['gain', '수익'], ['game', '놀이'], ['gift', '선물'], ['girl', '소녀'], ['give', '주다'], ['goal', '목적'], ['gray', '회색'], ['hair', '머리'], ['harm', '피해'], ['hell', '지옥'], ['help', '도움'], ['high', '높은'], ['hole', '구멍'], ['hope', '소망'], ['hour', '시각'], ['join', '합류'], ['kiss', '키스'], ['knee', '무릎'], ['lady', '부인'], ['late', '늦은'], ['left', '왼쪽'], ['life', '생명'], ['loss', '손해'], ['love', '사랑'], ['luck', '행운'], ['mail', '우편'], ['male', '남자'], ['many', '많은'], ['meal', '식사'], ['meat', '고기'], ['menu', '메뉴'], ['milk', '우유'], ['name', '이름'], ['news', '뉴스'], ['next', '다음'], ['once', '한번'], ['pain', '고통'], ['part', '부분'], ['pick', '선택'], ['pink', '핑크'], ['plan', '계획'], ['read', '읽다'], ['rest', '휴식'], ['ring', '반지'], ['rock', '바위'], ['roof', '지붕'], ['root', '뿌리'], ['rose', '장미'], ['rule', '규칙'], ['salt', '소금'], ['sand', '모래'], ['sell', '팔다'], ['shop', '가게'], ['sign', '싸인'], ['size', '크기'], ['skin', '피부'], ['slow', '늦다'], ['song', '노래'], ['soon', '금방'], ['tall', '크다'], ['test', '시험'], ['them', '그들'], ['thin', '얇은'], ['this', '이것'], ['time', '시간'], ['tiny', '작은'], ['tool', '도구'], ['tour', '관광'], ['tree', '나무'], ['trip', '여행'], ['very', '매우'], ['wave', '파도'], ['weak', '약한'], ['wear', '입다'], ['west', '서쪽'], ['when', '언제'], ['wide', '넓은'], ['wife', '아내'], ['wind', '바람'], ['wing', '날개'], ['wish', '바램'], ['wood', '나무'], ['word', '단어'], ['year', '연도']]\n",
      "총 단어 수:  110\n"
     ]
    }
   ],
   "source": [
    "raw = pd.read_csv('datasets/translate.csv', header=None)\n",
    "eng_kor = raw.values.tolist()\n",
    "\n",
    "print('사전 내용\\n', eng_kor)\n",
    "print('총 단어 수: ', len(eng_kor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어와 한글 인코딩 입력, 디코딩 입력, 디코딩 출력으로 변환\n",
    "def encode(eng_kor):\n",
    "    enc_in = []\n",
    "    dec_in = []\n",
    "    rnn_out = []\n",
    "\n",
    "    for seq in eng_kor:\n",
    "        eng = [char_to_num[c] for c in seq[0]]\n",
    "        enc_in.append(np.eye(n_input)[eng])\n",
    "\n",
    "        kor = [char_to_num[c] for c in ('S'+seq[1])]\n",
    "        dec_in.append(np.eye(n_input)[kor])\n",
    "\n",
    "        target = [char_to_num[c] for c in (seq[1]+'E')]\n",
    "        rnn_out.append(target)\n",
    "\n",
    "    enc_in = np.array(enc_in)\n",
    "    dec_in = np.array(dec_in)\n",
    "    rnn_out = np.array(rnn_out)\n",
    "\n",
    "    # 3차원 구조로 변환\n",
    "    rnn_out = np.expand_dims(rnn_out, axis=2)\n",
    "\n",
    "    return enc_in, dec_in, rnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [['word', '단어']]\n",
    "enc_in, dec_in, rnn_out = encode(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "인코더 입력 값 샘플\n",
      "데이터 모양: (1, 4, 171)\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "w\n",
      "o\n",
      "r\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "# 인코더 입력 값 샘플 출력\n",
    "print('\\n인코더 입력 값 샘플')\n",
    "print('데이터 모양:', enc_in.shape)\n",
    "print(enc_in)\n",
    "\n",
    "for i in range(4):\n",
    "    char = np.argmax(enc_in[0, i])\n",
    "    print(num_to_char[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디코더 입력 값 샘플\n",
      "데이터 모양: (1, 3, 171)\n",
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "S\n",
      "단\n",
      "어\n"
     ]
    }
   ],
   "source": [
    "# 디코더 입력 값 샘플 출력\n",
    "print('\\n디코더 입력 값 샘플')\n",
    "print('데이터 모양:', dec_in.shape)\n",
    "print(dec_in)\n",
    "\n",
    "for i in range(3):\n",
    "    char = np.argmax(dec_in[0, i])\n",
    "    print(num_to_char[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RNN 출력 값 샘플\n",
      "데이터 모양: (1, 3, 1)\n",
      "[[[ 61]\n",
      "  [114]\n",
      "  [  1]]]\n",
      "단\n",
      "어\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# RNN 출력 값 샘플 출력\n",
    "print('\\nRNN 출력 값 샘플')\n",
    "print('데이터 모양:', rnn_out.shape)\n",
    "print(rnn_out)\n",
    "\n",
    "for i in range(3):\n",
    "    num = rnn_out[0, i, 0]\n",
    "    print(num_to_char[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더 입력값 모양 (110, 4, 171)\n",
      "디코더 입력값 모양 (110, 3, 171)\n",
      "출력값 모양 (110, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "X_enc, X_dec, Y_rnn = encode(eng_kor)\n",
    "\n",
    "print('인코더 입력값 모양', X_enc.shape)\n",
    "print('디코더 입력값 모양', X_dec.shape)\n",
    "print('출력값 모양', Y_rnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 171)]     0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 3, 171)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 128),        153600      ['input_1[0][0]']                \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 3, 128)       153600      ['input_2[0][0]',                \n",
      "                                                                  'lstm[0][1]',                   \n",
      "                                                                  'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 171)       22059       ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 329,259\n",
      "Trainable params: 329,259\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM을 이용한 인공신경망 모형 정의\n",
    "enc_IN = Input(shape=(4, n_input))\n",
    "_, state_h, state_c = LSTM(units=MY_HIDDEN, return_state=True)(enc_IN)\n",
    "\n",
    "link = [state_h, state_c]\n",
    "\n",
    "dec_IN = Input(shape=(3, n_input))\n",
    "dec_Y1 = LSTM(units=MY_HIDDEN, return_sequences=True)(dec_IN, initial_state=link)\n",
    "dec_Y2 = Dense(units=n_input, activation='softmax')(dec_Y1)\n",
    "\n",
    "model = Model(inputs=[enc_IN, dec_IN], outputs=dec_Y2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 [==============================] - 3s 6ms/step - loss: 5.1296\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.0867\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.0297\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 4.9334\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.7456\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.3477\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.6749\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 3.5566\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.5758\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.4000\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.3835\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.3816\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.3266\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.2990\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.2911\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.2681\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.2455\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.2328\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2176\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.2013\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.1877\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.1728\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.1573\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.1430\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.1275\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.1114\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.0962\n",
      "Epoch 28/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.0804\n",
      "Epoch 29/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0641\n",
      "Epoch 30/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.0483\n",
      "Epoch 31/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.0307\n",
      "Epoch 32/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.0140\n",
      "Epoch 33/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9964\n",
      "Epoch 34/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9792\n",
      "Epoch 35/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9619\n",
      "Epoch 36/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.9431\n",
      "Epoch 37/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9263\n",
      "Epoch 38/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9065\n",
      "Epoch 39/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.8877\n",
      "Epoch 40/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.8686\n",
      "Epoch 41/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.8478\n",
      "Epoch 42/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.8280\n",
      "Epoch 43/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.8078\n",
      "Epoch 44/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.7858\n",
      "Epoch 45/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.7646\n",
      "Epoch 46/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.7400\n",
      "Epoch 47/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.7149\n",
      "Epoch 48/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.6887\n",
      "Epoch 49/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.6584\n",
      "Epoch 50/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6246\n",
      "Epoch 51/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.5912\n",
      "Epoch 52/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.5518\n",
      "Epoch 53/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.5142\n",
      "Epoch 54/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.4715\n",
      "Epoch 55/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.4262\n",
      "Epoch 56/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.3837\n",
      "Epoch 57/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.3345\n",
      "Epoch 58/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2851\n",
      "Epoch 59/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.2335\n",
      "Epoch 60/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1779\n",
      "Epoch 61/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1248\n",
      "Epoch 62/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.0619\n",
      "Epoch 63/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.0027\n",
      "Epoch 64/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9440\n",
      "Epoch 65/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.8866\n",
      "Epoch 66/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.8221\n",
      "Epoch 67/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7589\n",
      "Epoch 68/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.6970\n",
      "Epoch 69/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.6305\n",
      "Epoch 70/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5706\n",
      "Epoch 71/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5108\n",
      "Epoch 72/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.4504\n",
      "Epoch 73/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3880\n",
      "Epoch 74/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3294\n",
      "Epoch 75/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.2753\n",
      "Epoch 76/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2200\n",
      "Epoch 77/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1685\n",
      "Epoch 78/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1183\n",
      "Epoch 79/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0639\n",
      "Epoch 80/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0186\n",
      "Epoch 81/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9708\n",
      "Epoch 82/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9311\n",
      "Epoch 83/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8893\n",
      "Epoch 84/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8497\n",
      "Epoch 85/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8106\n",
      "Epoch 86/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7710\n",
      "Epoch 87/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7355\n",
      "Epoch 88/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7020\n",
      "Epoch 89/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6674\n",
      "Epoch 90/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6397\n",
      "Epoch 91/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6084\n",
      "Epoch 92/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5824\n",
      "Epoch 93/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5562\n",
      "Epoch 94/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5308\n",
      "Epoch 95/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5088\n",
      "Epoch 96/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4854\n",
      "Epoch 97/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4663\n",
      "Epoch 98/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4465\n",
      "Epoch 99/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4261\n",
      "Epoch 100/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4083\n",
      "Epoch 101/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3914\n",
      "Epoch 102/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3760\n",
      "Epoch 103/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3604\n",
      "Epoch 104/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3459\n",
      "Epoch 105/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3317\n",
      "Epoch 106/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3196\n",
      "Epoch 107/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3065\n",
      "Epoch 108/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2955\n",
      "Epoch 109/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2860\n",
      "Epoch 110/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2743\n",
      "Epoch 111/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2656\n",
      "Epoch 112/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2556\n",
      "Epoch 113/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2453\n",
      "Epoch 114/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2371\n",
      "Epoch 115/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2289\n",
      "Epoch 116/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2216\n",
      "Epoch 117/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2142\n",
      "Epoch 118/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2082\n",
      "Epoch 119/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2008\n",
      "Epoch 120/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1947\n",
      "Epoch 121/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1885\n",
      "Epoch 122/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1828\n",
      "Epoch 123/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1765\n",
      "Epoch 124/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1724\n",
      "Epoch 125/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1661\n",
      "Epoch 126/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1623\n",
      "Epoch 127/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1571\n",
      "Epoch 128/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1521\n",
      "Epoch 129/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1480\n",
      "Epoch 130/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1431\n",
      "Epoch 131/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1401\n",
      "Epoch 132/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1364\n",
      "Epoch 133/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1328\n",
      "Epoch 134/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1286\n",
      "Epoch 135/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1263\n",
      "Epoch 136/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1230\n",
      "Epoch 137/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1198\n",
      "Epoch 138/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1169\n",
      "Epoch 139/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1136\n",
      "Epoch 140/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.1113\n",
      "Epoch 141/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1088\n",
      "Epoch 142/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1053\n",
      "Epoch 143/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1029\n",
      "Epoch 144/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1008\n",
      "Epoch 145/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0984\n",
      "Epoch 146/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0960\n",
      "Epoch 147/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0937\n",
      "Epoch 148/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0920\n",
      "Epoch 149/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0905\n",
      "Epoch 150/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0884\n",
      "Epoch 151/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0862\n",
      "Epoch 152/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0844\n",
      "Epoch 153/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0827\n",
      "Epoch 154/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0809\n",
      "Epoch 155/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0793\n",
      "Epoch 156/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0777\n",
      "Epoch 157/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0761\n",
      "Epoch 158/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0745\n",
      "Epoch 159/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0733\n",
      "Epoch 160/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0716\n",
      "Epoch 161/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0701\n",
      "Epoch 162/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0692\n",
      "Epoch 163/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0679\n",
      "Epoch 164/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0664\n",
      "Epoch 165/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0653\n",
      "Epoch 166/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0640\n",
      "Epoch 167/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0627\n",
      "Epoch 168/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0616\n",
      "Epoch 169/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0604\n",
      "Epoch 170/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0597\n",
      "Epoch 171/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0584\n",
      "Epoch 172/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0579\n",
      "Epoch 173/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0569\n",
      "Epoch 174/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0554\n",
      "Epoch 175/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0548\n",
      "Epoch 176/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0538\n",
      "Epoch 177/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0530\n",
      "Epoch 178/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0522\n",
      "Epoch 179/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0509\n",
      "Epoch 180/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0504\n",
      "Epoch 181/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0495\n",
      "Epoch 182/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0484\n",
      "Epoch 183/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0478\n",
      "Epoch 184/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0472\n",
      "Epoch 185/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0465\n",
      "Epoch 186/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0457\n",
      "Epoch 187/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0449\n",
      "Epoch 188/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0444\n",
      "Epoch 189/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0438\n",
      "Epoch 190/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0431\n",
      "Epoch 191/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0423\n",
      "Epoch 192/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0417\n",
      "Epoch 193/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0411\n",
      "Epoch 194/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0406\n",
      "Epoch 195/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0400\n",
      "Epoch 196/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0394\n",
      "Epoch 197/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0388\n",
      "Epoch 198/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0382\n",
      "Epoch 199/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0378\n",
      "Epoch 200/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0373\n",
      "Epoch 201/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0368\n",
      "Epoch 202/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0363\n",
      "Epoch 203/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0357\n",
      "Epoch 204/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0352\n",
      "Epoch 205/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0348\n",
      "Epoch 206/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0344\n",
      "Epoch 207/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0338\n",
      "Epoch 208/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0334\n",
      "Epoch 209/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0330\n",
      "Epoch 210/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0326\n",
      "Epoch 211/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0322\n",
      "Epoch 212/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0317\n",
      "Epoch 213/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0313\n",
      "Epoch 214/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0310\n",
      "Epoch 215/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0306\n",
      "Epoch 216/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0301\n",
      "Epoch 217/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0298\n",
      "Epoch 218/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0294\n",
      "Epoch 219/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0290\n",
      "Epoch 220/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0287\n",
      "Epoch 221/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0284\n",
      "Epoch 222/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0280\n",
      "Epoch 223/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0277\n",
      "Epoch 224/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0274\n",
      "Epoch 225/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0271\n",
      "Epoch 226/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0269\n",
      "Epoch 227/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0265\n",
      "Epoch 228/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0262\n",
      "Epoch 229/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0259\n",
      "Epoch 230/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0256\n",
      "Epoch 231/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0254\n",
      "Epoch 232/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0251\n",
      "Epoch 233/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0248\n",
      "Epoch 234/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0246\n",
      "Epoch 235/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0243\n",
      "Epoch 236/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0240\n",
      "Epoch 237/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0237\n",
      "Epoch 238/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0235\n",
      "Epoch 239/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0233\n",
      "Epoch 240/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0230\n",
      "Epoch 241/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0227\n",
      "Epoch 242/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0225\n",
      "Epoch 243/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0224\n",
      "Epoch 244/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0221\n",
      "Epoch 245/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0219\n",
      "Epoch 246/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0216\n",
      "Epoch 247/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0214\n",
      "Epoch 248/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0211\n",
      "Epoch 249/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0210\n",
      "Epoch 250/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0208\n",
      "Epoch 251/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0205\n",
      "Epoch 252/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0203\n",
      "Epoch 253/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0201\n",
      "Epoch 254/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0199\n",
      "Epoch 255/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0197\n",
      "Epoch 256/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0195\n",
      "Epoch 257/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0194\n",
      "Epoch 258/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0192\n",
      "Epoch 259/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0190\n",
      "Epoch 260/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0188\n",
      "Epoch 261/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0186\n",
      "Epoch 262/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0184\n",
      "Epoch 263/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0183\n",
      "Epoch 264/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0181\n",
      "Epoch 265/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0179\n",
      "Epoch 266/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0178\n",
      "Epoch 267/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0176\n",
      "Epoch 268/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0175\n",
      "Epoch 269/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0173\n",
      "Epoch 270/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0171\n",
      "Epoch 271/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0170\n",
      "Epoch 272/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0168\n",
      "Epoch 273/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0167\n",
      "Epoch 274/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0165\n",
      "Epoch 275/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0164\n",
      "Epoch 276/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0162\n",
      "Epoch 277/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0161\n",
      "Epoch 278/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0159\n",
      "Epoch 279/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0158\n",
      "Epoch 280/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0157\n",
      "Epoch 281/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0155\n",
      "Epoch 282/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0154\n",
      "Epoch 283/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0153\n",
      "Epoch 284/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0152\n",
      "Epoch 285/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0151\n",
      "Epoch 286/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0149\n",
      "Epoch 287/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0148\n",
      "Epoch 288/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0146\n",
      "Epoch 289/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0145\n",
      "Epoch 290/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0145\n",
      "Epoch 291/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0143\n",
      "Epoch 292/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0142\n",
      "Epoch 293/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0141\n",
      "Epoch 294/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0139\n",
      "Epoch 295/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0138\n",
      "Epoch 296/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0137\n",
      "Epoch 297/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0136\n",
      "Epoch 298/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0135\n",
      "Epoch 299/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0134\n",
      "Epoch 300/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0133\n",
      "Epoch 301/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0132\n",
      "Epoch 302/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0131\n",
      "Epoch 303/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0130\n",
      "Epoch 304/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0129\n",
      "Epoch 305/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0128\n",
      "Epoch 306/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0127\n",
      "Epoch 307/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0125\n",
      "Epoch 308/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0125\n",
      "Epoch 309/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0124\n",
      "Epoch 310/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0123\n",
      "Epoch 311/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0122\n",
      "Epoch 312/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0121\n",
      "Epoch 313/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0120\n",
      "Epoch 314/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0119\n",
      "Epoch 315/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0118\n",
      "Epoch 316/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0117\n",
      "Epoch 317/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0116\n",
      "Epoch 318/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0116\n",
      "Epoch 319/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0115\n",
      "Epoch 320/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0114\n",
      "Epoch 321/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0113\n",
      "Epoch 322/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0112\n",
      "Epoch 323/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0111\n",
      "Epoch 324/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0111\n",
      "Epoch 325/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0110\n",
      "Epoch 326/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0109\n",
      "Epoch 327/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0108\n",
      "Epoch 328/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0108\n",
      "Epoch 329/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0107\n",
      "Epoch 330/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0106\n",
      "Epoch 331/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0105\n",
      "Epoch 332/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0104\n",
      "Epoch 333/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0104\n",
      "Epoch 334/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0103\n",
      "Epoch 335/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0102\n",
      "Epoch 336/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0102\n",
      "Epoch 337/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0101\n",
      "Epoch 338/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0100\n",
      "Epoch 339/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0100\n",
      "Epoch 340/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0099\n",
      "Epoch 341/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0098\n",
      "Epoch 342/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0098\n",
      "Epoch 343/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0097\n",
      "Epoch 344/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0096\n",
      "Epoch 345/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0095\n",
      "Epoch 346/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0095\n",
      "Epoch 347/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0094\n",
      "Epoch 348/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0094\n",
      "Epoch 349/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0093\n",
      "Epoch 350/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0092\n",
      "Epoch 351/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0092\n",
      "Epoch 352/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0091\n",
      "Epoch 353/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0091\n",
      "Epoch 354/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0090\n",
      "Epoch 355/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0089\n",
      "Epoch 356/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0089\n",
      "Epoch 357/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0088\n",
      "Epoch 358/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0088\n",
      "Epoch 359/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0087\n",
      "Epoch 360/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0087\n",
      "Epoch 361/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0086\n",
      "Epoch 362/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0086\n",
      "Epoch 363/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0085\n",
      "Epoch 364/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0084\n",
      "Epoch 365/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0084\n",
      "Epoch 366/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0083\n",
      "Epoch 367/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0083\n",
      "Epoch 368/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0082\n",
      "Epoch 369/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0082\n",
      "Epoch 370/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0081\n",
      "Epoch 371/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0081\n",
      "Epoch 372/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0080\n",
      "Epoch 373/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0080\n",
      "Epoch 374/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0079\n",
      "Epoch 375/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0079\n",
      "Epoch 376/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0078\n",
      "Epoch 377/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0078\n",
      "Epoch 378/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0077\n",
      "Epoch 379/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0077\n",
      "Epoch 380/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0077\n",
      "Epoch 381/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0076\n",
      "Epoch 382/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0076\n",
      "Epoch 383/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0075\n",
      "Epoch 384/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0075\n",
      "Epoch 385/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0074\n",
      "Epoch 386/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0074\n",
      "Epoch 387/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0073\n",
      "Epoch 388/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0073\n",
      "Epoch 389/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0072\n",
      "Epoch 390/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 391/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0072\n",
      "Epoch 392/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0071\n",
      "Epoch 393/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0071\n",
      "Epoch 394/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0070\n",
      "Epoch 395/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0070\n",
      "Epoch 396/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0070\n",
      "Epoch 397/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0069\n",
      "Epoch 398/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0069\n",
      "Epoch 399/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0068\n",
      "Epoch 400/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0068\n",
      "Epoch 401/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0068\n",
      "Epoch 402/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0067\n",
      "Epoch 403/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0067\n",
      "Epoch 404/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 405/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 406/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 407/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0065\n",
      "Epoch 408/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0065\n",
      "Epoch 409/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0065\n",
      "Epoch 410/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 411/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0064\n",
      "Epoch 412/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0064\n",
      "Epoch 413/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0063\n",
      "Epoch 414/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0063\n",
      "Epoch 415/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0063\n",
      "Epoch 416/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 417/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 418/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0062\n",
      "Epoch 419/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0061\n",
      "Epoch 420/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0061\n",
      "Epoch 421/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0061\n",
      "Epoch 422/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0060\n",
      "Epoch 423/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0060\n",
      "Epoch 424/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 425/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 426/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0059\n",
      "Epoch 427/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0059\n",
      "Epoch 428/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 429/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 430/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0058\n",
      "Epoch 431/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0057\n",
      "Epoch 432/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0057\n",
      "Epoch 433/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0057\n",
      "Epoch 434/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0057\n",
      "Epoch 435/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0056\n",
      "Epoch 436/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0056\n",
      "Epoch 437/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0056\n",
      "Epoch 438/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0055\n",
      "Epoch 439/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0055\n",
      "Epoch 440/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0055\n",
      "Epoch 441/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0055\n",
      "Epoch 442/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0054\n",
      "Epoch 443/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0054\n",
      "Epoch 444/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0054\n",
      "Epoch 445/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0053\n",
      "Epoch 446/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0053\n",
      "Epoch 447/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0053\n",
      "Epoch 448/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0053\n",
      "Epoch 449/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0052\n",
      "Epoch 450/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0052\n",
      "Epoch 451/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0052\n",
      "Epoch 452/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0052\n",
      "Epoch 453/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0051\n",
      "Epoch 454/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0051\n",
      "Epoch 455/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0051\n",
      "Epoch 456/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0051\n",
      "Epoch 457/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0050\n",
      "Epoch 458/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0050\n",
      "Epoch 459/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0050\n",
      "Epoch 460/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0050\n",
      "Epoch 461/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0049\n",
      "Epoch 462/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0049\n",
      "Epoch 463/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0049\n",
      "Epoch 464/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0049\n",
      "Epoch 465/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0048\n",
      "Epoch 466/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0048\n",
      "Epoch 467/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0048\n",
      "Epoch 468/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0048\n",
      "Epoch 469/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0047\n",
      "Epoch 470/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0047\n",
      "Epoch 471/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0047\n",
      "Epoch 472/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0047\n",
      "Epoch 473/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0047\n",
      "Epoch 474/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0046\n",
      "Epoch 475/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0046\n",
      "Epoch 476/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0046\n",
      "Epoch 477/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0046\n",
      "Epoch 478/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0045\n",
      "Epoch 479/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0045\n",
      "Epoch 480/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0045\n",
      "Epoch 481/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0045\n",
      "Epoch 482/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0045\n",
      "Epoch 483/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0044\n",
      "Epoch 484/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0044\n",
      "Epoch 485/500\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0044\n",
      "Epoch 486/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0044\n",
      "Epoch 487/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0044\n",
      "Epoch 488/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0043\n",
      "Epoch 489/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0043\n",
      "Epoch 490/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0043\n",
      "Epoch 491/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0043\n",
      "Epoch 492/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0043\n",
      "Epoch 493/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0042\n",
      "Epoch 494/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0042\n",
      "Epoch 495/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0042\n",
      "Epoch 496/500\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0042\n",
      "Epoch 497/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0042\n",
      "Epoch 498/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
      "Epoch 499/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
      "Epoch 500/500\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
      "training time:  14.75초\n"
     ]
    }
   ],
   "source": [
    "begin = time()\n",
    "model.fit([X_enc, X_dec], Y_rnn, epochs=MY_EPOCHS)\n",
    "end = time()\n",
    "print(f'training time: {(end-begin): .2f}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick = randint(0, len(eng_kor), 5)\n",
    "choose = [eng_kor[i] for i in pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rest', '휴식'],\n",
       " ['male', '남자'],\n",
       " ['when', '언제'],\n",
       " ['menu', '메뉴'],\n",
       " ['wife', '아내'],\n",
       " ['love', 'PP'],\n",
       " ['olve', 'PP'],\n",
       " ['lvoe', 'PP'],\n",
       " ['loev', 'PP'],\n",
       " ['eovl', 'PP']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_this = [['love', 'PP'], ['olve', 'PP'], ['lvoe', 'PP'], ['loev', 'PP'], ['eovl', 'PP']]\n",
    "new_data = choose + try_this\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과의 모양: (10, 3, 171)\n"
     ]
    }
   ],
   "source": [
    "enc_in, dec_in, _ = encode(new_data)\n",
    "pred = model.predict([enc_in, dec_in])\n",
    "print(f'예측 결과의 모양: {pred.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rest => 휴식\n",
      "male => 남자\n",
      "when => 언제\n",
      "menu => 메뉴\n",
      "wife => 아내\n",
      "love => 사랑\n",
      "olve => 사E\n",
      "lvoe => 사사\n",
      "loev => 사E\n",
      "eovl => 도리\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과에서 한글 추출\n",
    "for i in range(len(new_data)):\n",
    "    eng = new_data[i][0]\n",
    "    word = np.argmax(pred[i], axis=1)\n",
    "    kor = ''\n",
    "    for j in range(2):\n",
    "        kor = kor + num_to_char[word[j]]\n",
    "    \n",
    "    print(eng, '=>', kor)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c726b45dd7293eb31a34b8969b40a947eb51be7b196d382e5dff74d4c7ac181"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('saltlux_deep_lecture': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
