{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 save() 함수를 이용한 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X, test_X = train_X / 255.0, test_X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(360, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 1.0921 - accuracy: 0.7106\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.4704 - accuracy: 0.8689\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3798 - accuracy: 0.8924\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3358 - accuracy: 0.9040\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.3056 - accuracy: 0.9118\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, batch_size=100, epochs=5, verbose=1)\n",
    "model.save('mnist_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 모델의 구조 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 구조만 저장\n",
    "model_json = model.to_json()\n",
    "with open('mnist_model.json', 'w') as f:\n",
    "    f.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 360)               282600    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               46208     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 330,098\n",
      "Trainable params: 330,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 불러오기\n",
    "with open('mnist_model.json', 'r') as f:\n",
    "    model_json = f.read()\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "model = model_from_json(model_json)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2501 - accuracy: 0.9285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2500655949115753, 0.9284999966621399]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 구조 있는 상태에서 가중치만 불러오기\n",
    "model.load_weights('mnist_keras_model.h5')\n",
    "model.compile(optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 모델의 구조와 가중치 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('mnist_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_X, train_y),(test_X, test_y) = mnist.load_data()\n",
    "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
    "\n",
    "import numpy as np\n",
    "pred = np.argmax(model.predict(test_X), axis=1)\n",
    "np.mean(np.equal(test_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 콜백을 이용한 딥러닝 모형 관리 및 모니터링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 모델 자동 저장 콜백 - ModelCheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9345\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.94517, saving model to model_dataset\\model-01-0.9452.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2266 - accuracy: 0.9345 - val_loss: 0.1921 - val_accuracy: 0.9452\n",
      "Epoch 2/50\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.2240 - accuracy: 0.9362\n",
      "Epoch 00002: val_accuracy improved from 0.94517 to 0.94533, saving model to model_dataset\\model-02-0.9453.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2243 - accuracy: 0.9363 - val_loss: 0.1912 - val_accuracy: 0.9453\n",
      "Epoch 3/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2273 - accuracy: 0.9357\n",
      "Epoch 00003: val_accuracy improved from 0.94533 to 0.94542, saving model to model_dataset\\model-03-0.9454.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2265 - accuracy: 0.9359 - val_loss: 0.1905 - val_accuracy: 0.9454\n",
      "Epoch 4/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2243 - accuracy: 0.9354\n",
      "Epoch 00004: val_accuracy improved from 0.94542 to 0.94600, saving model to model_dataset\\model-04-0.9460.h5\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.2245 - accuracy: 0.9355 - val_loss: 0.1899 - val_accuracy: 0.9460\n",
      "Epoch 5/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9356\n",
      "Epoch 00005: val_accuracy improved from 0.94600 to 0.94608, saving model to model_dataset\\model-05-0.9461.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2239 - accuracy: 0.9356 - val_loss: 0.1892 - val_accuracy: 0.9461\n",
      "Epoch 6/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2214 - accuracy: 0.9364\n",
      "Epoch 00006: val_accuracy improved from 0.94608 to 0.94642, saving model to model_dataset\\model-06-0.9464.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2214 - accuracy: 0.9364 - val_loss: 0.1885 - val_accuracy: 0.9464\n",
      "Epoch 7/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2228 - accuracy: 0.9363\n",
      "Epoch 00007: val_accuracy improved from 0.94642 to 0.94650, saving model to model_dataset\\model-07-0.9465.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2229 - accuracy: 0.9363 - val_loss: 0.1878 - val_accuracy: 0.9465\n",
      "Epoch 8/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2197 - accuracy: 0.9368\n",
      "Epoch 00008: val_accuracy improved from 0.94650 to 0.94658, saving model to model_dataset\\model-08-0.9466.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2194 - accuracy: 0.9369 - val_loss: 0.1872 - val_accuracy: 0.9466\n",
      "Epoch 9/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9367\n",
      "Epoch 00009: val_accuracy improved from 0.94658 to 0.94733, saving model to model_dataset\\model-09-0.9473.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2204 - accuracy: 0.9367 - val_loss: 0.1865 - val_accuracy: 0.9473\n",
      "Epoch 10/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2172 - accuracy: 0.9374\n",
      "Epoch 00010: val_accuracy improved from 0.94733 to 0.94750, saving model to model_dataset\\model-10-0.9475.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2180 - accuracy: 0.9372 - val_loss: 0.1859 - val_accuracy: 0.9475\n",
      "Epoch 11/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2188 - accuracy: 0.9369\n",
      "Epoch 00011: val_accuracy improved from 0.94750 to 0.94800, saving model to model_dataset\\model-11-0.9480.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2189 - accuracy: 0.9372 - val_loss: 0.1853 - val_accuracy: 0.9480\n",
      "Epoch 12/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2159 - accuracy: 0.9378\n",
      "Epoch 00012: val_accuracy did not improve from 0.94800\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2162 - accuracy: 0.9377 - val_loss: 0.1846 - val_accuracy: 0.9479\n",
      "Epoch 13/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2137 - accuracy: 0.9385\n",
      "Epoch 00013: val_accuracy improved from 0.94800 to 0.94842, saving model to model_dataset\\model-13-0.9484.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2143 - accuracy: 0.9383 - val_loss: 0.1839 - val_accuracy: 0.9484\n",
      "Epoch 14/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9398\n",
      "Epoch 00014: val_accuracy did not improve from 0.94842\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2138 - accuracy: 0.9398 - val_loss: 0.1831 - val_accuracy: 0.9483\n",
      "Epoch 15/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2137 - accuracy: 0.9385\n",
      "Epoch 00015: val_accuracy did not improve from 0.94842\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2144 - accuracy: 0.9383 - val_loss: 0.1827 - val_accuracy: 0.9484\n",
      "Epoch 16/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2125 - accuracy: 0.9380\n",
      "Epoch 00016: val_accuracy did not improve from 0.94842\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2127 - accuracy: 0.9380 - val_loss: 0.1821 - val_accuracy: 0.9482\n",
      "Epoch 17/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2127 - accuracy: 0.9397\n",
      "Epoch 00017: val_accuracy improved from 0.94842 to 0.94883, saving model to model_dataset\\model-17-0.9488.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2124 - accuracy: 0.9397 - val_loss: 0.1816 - val_accuracy: 0.9488\n",
      "Epoch 18/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2123 - accuracy: 0.9393\n",
      "Epoch 00018: val_accuracy improved from 0.94883 to 0.94892, saving model to model_dataset\\model-18-0.9489.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2123 - accuracy: 0.9393 - val_loss: 0.1810 - val_accuracy: 0.9489\n",
      "Epoch 19/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2105 - accuracy: 0.9396\n",
      "Epoch 00019: val_accuracy improved from 0.94892 to 0.94933, saving model to model_dataset\\model-19-0.9493.h5\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.2097 - accuracy: 0.9397 - val_loss: 0.1803 - val_accuracy: 0.9493\n",
      "Epoch 20/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2090 - accuracy: 0.9403\n",
      "Epoch 00020: val_accuracy did not improve from 0.94933\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2097 - accuracy: 0.9399 - val_loss: 0.1798 - val_accuracy: 0.9493\n",
      "Epoch 21/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2097 - accuracy: 0.9395\n",
      "Epoch 00021: val_accuracy did not improve from 0.94933\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2099 - accuracy: 0.9394 - val_loss: 0.1792 - val_accuracy: 0.9492\n",
      "Epoch 22/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2066 - accuracy: 0.9404\n",
      "Epoch 00022: val_accuracy improved from 0.94933 to 0.94950, saving model to model_dataset\\model-22-0.9495.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2068 - accuracy: 0.9404 - val_loss: 0.1785 - val_accuracy: 0.9495\n",
      "Epoch 23/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2091 - accuracy: 0.9407\n",
      "Epoch 00023: val_accuracy improved from 0.94950 to 0.95017, saving model to model_dataset\\model-23-0.9502.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2076 - accuracy: 0.9410 - val_loss: 0.1780 - val_accuracy: 0.9502\n",
      "Epoch 24/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2065 - accuracy: 0.9402\n",
      "Epoch 00024: val_accuracy did not improve from 0.95017\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2063 - accuracy: 0.9403 - val_loss: 0.1776 - val_accuracy: 0.9495\n",
      "Epoch 25/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2069 - accuracy: 0.9410\n",
      "Epoch 00025: val_accuracy did not improve from 0.95017\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2062 - accuracy: 0.9412 - val_loss: 0.1769 - val_accuracy: 0.9501\n",
      "Epoch 26/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2046 - accuracy: 0.9412\n",
      "Epoch 00026: val_accuracy improved from 0.95017 to 0.95042, saving model to model_dataset\\model-26-0.9504.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2041 - accuracy: 0.9414 - val_loss: 0.1763 - val_accuracy: 0.9504\n",
      "Epoch 27/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2045 - accuracy: 0.9417\n",
      "Epoch 00027: val_accuracy improved from 0.95042 to 0.95092, saving model to model_dataset\\model-27-0.9509.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2049 - accuracy: 0.9414 - val_loss: 0.1758 - val_accuracy: 0.9509\n",
      "Epoch 28/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2034 - accuracy: 0.9421\n",
      "Epoch 00028: val_accuracy did not improve from 0.95092\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2031 - accuracy: 0.9418 - val_loss: 0.1752 - val_accuracy: 0.9507\n",
      "Epoch 29/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9423\n",
      "Epoch 00029: val_accuracy did not improve from 0.95092\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2028 - accuracy: 0.9422 - val_loss: 0.1747 - val_accuracy: 0.9508\n",
      "Epoch 30/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2026 - accuracy: 0.9421\n",
      "Epoch 00030: val_accuracy improved from 0.95092 to 0.95117, saving model to model_dataset\\model-30-0.9512.h5\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.2027 - accuracy: 0.9422 - val_loss: 0.1742 - val_accuracy: 0.9512\n",
      "Epoch 31/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2003 - accuracy: 0.9426\n",
      "Epoch 00031: val_accuracy did not improve from 0.95117\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1998 - accuracy: 0.9426 - val_loss: 0.1736 - val_accuracy: 0.9512\n",
      "Epoch 32/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9412\n",
      "Epoch 00032: val_accuracy improved from 0.95117 to 0.95158, saving model to model_dataset\\model-32-0.9516.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2014 - accuracy: 0.9412 - val_loss: 0.1732 - val_accuracy: 0.9516\n",
      "Epoch 33/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1994 - accuracy: 0.9433\n",
      "Epoch 00033: val_accuracy improved from 0.95158 to 0.95192, saving model to model_dataset\\model-33-0.9519.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1994 - accuracy: 0.9433 - val_loss: 0.1725 - val_accuracy: 0.9519\n",
      "Epoch 34/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2005 - accuracy: 0.9422\n",
      "Epoch 00034: val_accuracy did not improve from 0.95192\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.2006 - accuracy: 0.9424 - val_loss: 0.1720 - val_accuracy: 0.9516\n",
      "Epoch 35/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9429\n",
      "Epoch 00035: val_accuracy improved from 0.95192 to 0.95225, saving model to model_dataset\\model-35-0.9523.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.1990 - accuracy: 0.9429 - val_loss: 0.1716 - val_accuracy: 0.9523\n",
      "Epoch 36/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1962 - accuracy: 0.9443\n",
      "Epoch 00036: val_accuracy improved from 0.95225 to 0.95250, saving model to model_dataset\\model-36-0.9525.h5\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1957 - accuracy: 0.9442 - val_loss: 0.1710 - val_accuracy: 0.9525\n",
      "Epoch 37/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1960 - accuracy: 0.9440\n",
      "Epoch 00037: val_accuracy did not improve from 0.95250\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1983 - accuracy: 0.9432 - val_loss: 0.1705 - val_accuracy: 0.9525\n",
      "Epoch 38/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9435\n",
      "Epoch 00038: val_accuracy improved from 0.95250 to 0.95283, saving model to model_dataset\\model-38-0.9528.h5\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1962 - accuracy: 0.9435 - val_loss: 0.1701 - val_accuracy: 0.9528\n",
      "Epoch 39/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9442\n",
      "Epoch 00039: val_accuracy improved from 0.95283 to 0.95317, saving model to model_dataset\\model-39-0.9532.h5\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1948 - accuracy: 0.9442 - val_loss: 0.1696 - val_accuracy: 0.9532\n",
      "Epoch 40/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1945 - accuracy: 0.9451\n",
      "Epoch 00040: val_accuracy did not improve from 0.95317\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1945 - accuracy: 0.9450 - val_loss: 0.1690 - val_accuracy: 0.9529\n",
      "Epoch 41/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9450\n",
      "Epoch 00041: val_accuracy did not improve from 0.95317\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1936 - accuracy: 0.9450 - val_loss: 0.1686 - val_accuracy: 0.9530\n",
      "Epoch 42/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1917 - accuracy: 0.9448\n",
      "Epoch 00042: val_accuracy did not improve from 0.95317\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1931 - accuracy: 0.9445 - val_loss: 0.1681 - val_accuracy: 0.9531\n",
      "Epoch 43/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9439\n",
      "Epoch 00043: val_accuracy improved from 0.95317 to 0.95367, saving model to model_dataset\\model-43-0.9537.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1928 - accuracy: 0.9441 - val_loss: 0.1676 - val_accuracy: 0.9537\n",
      "Epoch 44/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9445\n",
      "Epoch 00044: val_accuracy did not improve from 0.95367\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.1920 - accuracy: 0.9448 - val_loss: 0.1671 - val_accuracy: 0.9535\n",
      "Epoch 45/50\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9438\n",
      "Epoch 00045: val_accuracy did not improve from 0.95367\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1920 - accuracy: 0.9439 - val_loss: 0.1666 - val_accuracy: 0.9536\n",
      "Epoch 46/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1922 - accuracy: 0.9442\n",
      "Epoch 00046: val_accuracy did not improve from 0.95367\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1917 - accuracy: 0.9441 - val_loss: 0.1661 - val_accuracy: 0.9536\n",
      "Epoch 47/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1898 - accuracy: 0.9458\n",
      "Epoch 00047: val_accuracy improved from 0.95367 to 0.95375, saving model to model_dataset\\model-47-0.9538.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1899 - accuracy: 0.9460 - val_loss: 0.1656 - val_accuracy: 0.9538\n",
      "Epoch 48/50\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.1902 - accuracy: 0.9459\n",
      "Epoch 00048: val_accuracy improved from 0.95375 to 0.95442, saving model to model_dataset\\model-48-0.9544.h5\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1907 - accuracy: 0.9458 - val_loss: 0.1653 - val_accuracy: 0.9544\n",
      "Epoch 49/50\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.1888 - accuracy: 0.9455\n",
      "Epoch 00049: val_accuracy improved from 0.95442 to 0.95450, saving model to model_dataset\\model-49-0.9545.h5\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.1896 - accuracy: 0.9451 - val_loss: 0.1647 - val_accuracy: 0.9545\n",
      "Epoch 50/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1881 - accuracy: 0.9459\n",
      "Epoch 00050: val_accuracy did not improve from 0.95450\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.1881 - accuracy: 0.9459 - val_loss: 0.1643 - val_accuracy: 0.9543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2732d6ace80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "path = 'model_dataset/model-{epoch:02d}-{val_accuracy:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=path, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "model.fit(train_X, train_y, validation_split=0.2, batch_size=1000, epochs=50, callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 조기 종료 콜백 - EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1876 - accuracy: 0.9462 - val_loss: 0.1667 - val_accuracy: 0.9490\n",
      "Epoch 2/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1852 - accuracy: 0.9468 - val_loss: 0.1662 - val_accuracy: 0.9496\n",
      "Epoch 3/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1863 - accuracy: 0.9467 - val_loss: 0.1654 - val_accuracy: 0.9501\n",
      "Epoch 4/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1850 - accuracy: 0.9463 - val_loss: 0.1648 - val_accuracy: 0.9504\n",
      "Epoch 5/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1835 - accuracy: 0.9467 - val_loss: 0.1642 - val_accuracy: 0.9503\n",
      "Epoch 6/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1834 - accuracy: 0.9478 - val_loss: 0.1636 - val_accuracy: 0.9511\n",
      "Epoch 7/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1834 - accuracy: 0.9469 - val_loss: 0.1629 - val_accuracy: 0.9508\n",
      "Epoch 8/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1816 - accuracy: 0.9473 - val_loss: 0.1623 - val_accuracy: 0.9516\n",
      "Epoch 9/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1808 - accuracy: 0.9484 - val_loss: 0.1617 - val_accuracy: 0.9510\n",
      "Epoch 10/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1810 - accuracy: 0.9481 - val_loss: 0.1611 - val_accuracy: 0.9518\n",
      "Epoch 11/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1795 - accuracy: 0.9495 - val_loss: 0.1605 - val_accuracy: 0.9520\n",
      "Epoch 12/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1784 - accuracy: 0.9484 - val_loss: 0.1599 - val_accuracy: 0.9520\n",
      "Epoch 13/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1779 - accuracy: 0.9488 - val_loss: 0.1593 - val_accuracy: 0.9518\n",
      "Epoch 14/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1770 - accuracy: 0.9486 - val_loss: 0.1587 - val_accuracy: 0.9521\n",
      "Epoch 15/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1775 - accuracy: 0.9495 - val_loss: 0.1581 - val_accuracy: 0.9521\n",
      "Epoch 16/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1773 - accuracy: 0.9488 - val_loss: 0.1577 - val_accuracy: 0.9525\n",
      "Epoch 17/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1750 - accuracy: 0.9498 - val_loss: 0.1571 - val_accuracy: 0.9526\n",
      "Epoch 18/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1750 - accuracy: 0.9501 - val_loss: 0.1566 - val_accuracy: 0.9525\n",
      "Epoch 19/1000\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1743 - accuracy: 0.9500 - val_loss: 0.1561 - val_accuracy: 0.9524\n",
      "Epoch 20/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1723 - accuracy: 0.9510 - val_loss: 0.1555 - val_accuracy: 0.9529\n",
      "Epoch 21/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1727 - accuracy: 0.9503 - val_loss: 0.1550 - val_accuracy: 0.9528\n",
      "Epoch 22/1000\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1712 - accuracy: 0.9507 - val_loss: 0.1543 - val_accuracy: 0.9527\n",
      "Epoch 23/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1711 - accuracy: 0.9507 - val_loss: 0.1538 - val_accuracy: 0.9530\n",
      "Epoch 24/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1714 - accuracy: 0.9506 - val_loss: 0.1535 - val_accuracy: 0.9532\n",
      "Epoch 25/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1702 - accuracy: 0.9506 - val_loss: 0.1531 - val_accuracy: 0.9531\n",
      "Epoch 26/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1707 - accuracy: 0.9505 - val_loss: 0.1524 - val_accuracy: 0.9535\n",
      "Epoch 27/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1686 - accuracy: 0.9518 - val_loss: 0.1519 - val_accuracy: 0.9537\n",
      "Epoch 28/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1685 - accuracy: 0.9514 - val_loss: 0.1513 - val_accuracy: 0.9535\n",
      "Epoch 29/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1681 - accuracy: 0.9511 - val_loss: 0.1509 - val_accuracy: 0.9538\n",
      "Epoch 30/1000\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.1678 - accuracy: 0.9516 - val_loss: 0.1504 - val_accuracy: 0.9543\n",
      "Epoch 31/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1666 - accuracy: 0.9528 - val_loss: 0.1499 - val_accuracy: 0.9543\n",
      "Epoch 32/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1655 - accuracy: 0.9524 - val_loss: 0.1495 - val_accuracy: 0.9542\n",
      "Epoch 33/1000\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 0.1657 - accuracy: 0.9530 - val_loss: 0.1489 - val_accuracy: 0.9542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2732da007f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "model.fit(train_X, train_y, validation_data=(test_X, test_y), batch_size=1000,\n",
    "          epochs=1000, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# val_accuracy가 3회이상 증가 안하면 조기종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 학습 시각화 콜백 - Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60/60 [==============================] - 1s 20ms/step - loss: 0.1642 - accuracy: 0.9531 - val_loss: 0.1485 - val_accuracy: 0.9544\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1645 - accuracy: 0.9530 - val_loss: 0.1479 - val_accuracy: 0.9553\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1643 - accuracy: 0.9528 - val_loss: 0.1476 - val_accuracy: 0.9552\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1626 - accuracy: 0.9534 - val_loss: 0.1471 - val_accuracy: 0.9552\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1622 - accuracy: 0.9533 - val_loss: 0.1468 - val_accuracy: 0.9557\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1619 - accuracy: 0.9535 - val_loss: 0.1461 - val_accuracy: 0.9549\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1628 - accuracy: 0.9530 - val_loss: 0.1457 - val_accuracy: 0.9554\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1604 - accuracy: 0.9538 - val_loss: 0.1452 - val_accuracy: 0.9560\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1609 - accuracy: 0.9538 - val_loss: 0.1449 - val_accuracy: 0.9557\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1608 - accuracy: 0.9532 - val_loss: 0.1443 - val_accuracy: 0.9562\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1594 - accuracy: 0.9543 - val_loss: 0.1440 - val_accuracy: 0.9564\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1584 - accuracy: 0.9544 - val_loss: 0.1435 - val_accuracy: 0.9562\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1583 - accuracy: 0.9545 - val_loss: 0.1432 - val_accuracy: 0.9564\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1577 - accuracy: 0.9548 - val_loss: 0.1428 - val_accuracy: 0.9563\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1563 - accuracy: 0.9551 - val_loss: 0.1424 - val_accuracy: 0.9566\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1554 - accuracy: 0.9558 - val_loss: 0.1417 - val_accuracy: 0.9569\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1552 - accuracy: 0.9548 - val_loss: 0.1416 - val_accuracy: 0.9569\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1545 - accuracy: 0.9552 - val_loss: 0.1411 - val_accuracy: 0.9566\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1551 - accuracy: 0.9557 - val_loss: 0.1407 - val_accuracy: 0.9571\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1548 - accuracy: 0.9559 - val_loss: 0.1403 - val_accuracy: 0.9568\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1546 - accuracy: 0.9553 - val_loss: 0.1399 - val_accuracy: 0.9569\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1543 - accuracy: 0.9553 - val_loss: 0.1396 - val_accuracy: 0.9577\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1524 - accuracy: 0.9557 - val_loss: 0.1392 - val_accuracy: 0.9570\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1528 - accuracy: 0.9565 - val_loss: 0.1389 - val_accuracy: 0.9571\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1514 - accuracy: 0.9560 - val_loss: 0.1385 - val_accuracy: 0.9576\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1505 - accuracy: 0.9568 - val_loss: 0.1379 - val_accuracy: 0.9574\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1511 - accuracy: 0.9572 - val_loss: 0.1377 - val_accuracy: 0.9576\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1518 - accuracy: 0.9561 - val_loss: 0.1373 - val_accuracy: 0.9576\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1511 - accuracy: 0.9570 - val_loss: 0.1368 - val_accuracy: 0.9577\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1496 - accuracy: 0.9572 - val_loss: 0.1364 - val_accuracy: 0.9583\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1499 - accuracy: 0.9573 - val_loss: 0.1361 - val_accuracy: 0.9582\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1486 - accuracy: 0.9572 - val_loss: 0.1358 - val_accuracy: 0.9583\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1473 - accuracy: 0.9588 - val_loss: 0.1354 - val_accuracy: 0.9581\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1472 - accuracy: 0.9578 - val_loss: 0.1351 - val_accuracy: 0.9583\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1470 - accuracy: 0.9578 - val_loss: 0.1347 - val_accuracy: 0.9585\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1467 - accuracy: 0.9579 - val_loss: 0.1342 - val_accuracy: 0.9588\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1459 - accuracy: 0.9584 - val_loss: 0.1340 - val_accuracy: 0.9589\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1456 - accuracy: 0.9581 - val_loss: 0.1336 - val_accuracy: 0.9590\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 1s 19ms/step - loss: 0.1465 - accuracy: 0.9581 - val_loss: 0.1334 - val_accuracy: 0.9589\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1452 - accuracy: 0.9578 - val_loss: 0.1329 - val_accuracy: 0.9597\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1450 - accuracy: 0.9582 - val_loss: 0.1327 - val_accuracy: 0.9594\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1458 - accuracy: 0.9582 - val_loss: 0.1323 - val_accuracy: 0.9594\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1428 - accuracy: 0.9583 - val_loss: 0.1321 - val_accuracy: 0.9595\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1428 - accuracy: 0.9596 - val_loss: 0.1315 - val_accuracy: 0.9602\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1415 - accuracy: 0.9590 - val_loss: 0.1313 - val_accuracy: 0.9601\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1428 - accuracy: 0.9585 - val_loss: 0.1310 - val_accuracy: 0.9603\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1415 - accuracy: 0.9600 - val_loss: 0.1306 - val_accuracy: 0.9605\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1413 - accuracy: 0.9592 - val_loss: 0.1304 - val_accuracy: 0.9604\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1413 - accuracy: 0.9596 - val_loss: 0.1300 - val_accuracy: 0.9604\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1388 - accuracy: 0.9604 - val_loss: 0.1297 - val_accuracy: 0.9607\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1408 - accuracy: 0.9594 - val_loss: 0.1293 - val_accuracy: 0.9606\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1394 - accuracy: 0.9602 - val_loss: 0.1290 - val_accuracy: 0.9604\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1396 - accuracy: 0.9596 - val_loss: 0.1287 - val_accuracy: 0.9608\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1386 - accuracy: 0.9606 - val_loss: 0.1284 - val_accuracy: 0.9611\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 1s 19ms/step - loss: 0.1378 - accuracy: 0.9606 - val_loss: 0.1280 - val_accuracy: 0.9610\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1386 - accuracy: 0.9609 - val_loss: 0.1278 - val_accuracy: 0.9612\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1367 - accuracy: 0.9611 - val_loss: 0.1273 - val_accuracy: 0.9613\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1362 - accuracy: 0.9605 - val_loss: 0.1271 - val_accuracy: 0.9611\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1365 - accuracy: 0.9608 - val_loss: 0.1267 - val_accuracy: 0.9617\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1360 - accuracy: 0.9607 - val_loss: 0.1265 - val_accuracy: 0.9616\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1364 - accuracy: 0.9610 - val_loss: 0.1262 - val_accuracy: 0.9614\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1362 - accuracy: 0.9614 - val_loss: 0.1259 - val_accuracy: 0.9620\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1358 - accuracy: 0.9610 - val_loss: 0.1255 - val_accuracy: 0.9618\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1353 - accuracy: 0.9608 - val_loss: 0.1253 - val_accuracy: 0.9618\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1341 - accuracy: 0.9615 - val_loss: 0.1249 - val_accuracy: 0.9626\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1347 - accuracy: 0.9618 - val_loss: 0.1246 - val_accuracy: 0.9624\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1342 - accuracy: 0.9617 - val_loss: 0.1244 - val_accuracy: 0.9629\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1340 - accuracy: 0.9613 - val_loss: 0.1241 - val_accuracy: 0.9629\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1321 - accuracy: 0.9616 - val_loss: 0.1239 - val_accuracy: 0.9631\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1320 - accuracy: 0.9619 - val_loss: 0.1235 - val_accuracy: 0.9628\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1327 - accuracy: 0.9614 - val_loss: 0.1233 - val_accuracy: 0.9628\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1309 - accuracy: 0.9620 - val_loss: 0.1230 - val_accuracy: 0.9632\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1310 - accuracy: 0.9631 - val_loss: 0.1228 - val_accuracy: 0.9635\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1313 - accuracy: 0.9623 - val_loss: 0.1225 - val_accuracy: 0.9638\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1307 - accuracy: 0.9625 - val_loss: 0.1221 - val_accuracy: 0.9637\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1303 - accuracy: 0.9622 - val_loss: 0.1217 - val_accuracy: 0.9632\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1304 - accuracy: 0.9628 - val_loss: 0.1215 - val_accuracy: 0.9637\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1298 - accuracy: 0.9627 - val_loss: 0.1211 - val_accuracy: 0.9637\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1285 - accuracy: 0.9631 - val_loss: 0.1210 - val_accuracy: 0.9638\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1285 - accuracy: 0.9630 - val_loss: 0.1208 - val_accuracy: 0.9640\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1279 - accuracy: 0.9634 - val_loss: 0.1203 - val_accuracy: 0.9640\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1285 - accuracy: 0.9629 - val_loss: 0.1202 - val_accuracy: 0.9645\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1280 - accuracy: 0.9628 - val_loss: 0.1199 - val_accuracy: 0.9643\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1268 - accuracy: 0.9635 - val_loss: 0.1197 - val_accuracy: 0.9650\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 1s 18ms/step - loss: 0.1277 - accuracy: 0.9630 - val_loss: 0.1194 - val_accuracy: 0.9647\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1267 - accuracy: 0.9634 - val_loss: 0.1191 - val_accuracy: 0.9647\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1251 - accuracy: 0.9641 - val_loss: 0.1189 - val_accuracy: 0.9649\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1264 - accuracy: 0.9642 - val_loss: 0.1187 - val_accuracy: 0.9647\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1255 - accuracy: 0.9644 - val_loss: 0.1183 - val_accuracy: 0.9650\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1258 - accuracy: 0.9640 - val_loss: 0.1180 - val_accuracy: 0.9648\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1255 - accuracy: 0.9641 - val_loss: 0.1177 - val_accuracy: 0.9645\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1239 - accuracy: 0.9647 - val_loss: 0.1175 - val_accuracy: 0.9649\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1245 - accuracy: 0.9645 - val_loss: 0.1174 - val_accuracy: 0.9653\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1233 - accuracy: 0.9642 - val_loss: 0.1171 - val_accuracy: 0.9651\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1235 - accuracy: 0.9646 - val_loss: 0.1170 - val_accuracy: 0.9650\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1234 - accuracy: 0.9644 - val_loss: 0.1169 - val_accuracy: 0.9653\n",
      "Epoch 97/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1236 - accuracy: 0.9643 - val_loss: 0.1164 - val_accuracy: 0.9647\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1236 - accuracy: 0.9648 - val_loss: 0.1162 - val_accuracy: 0.9651\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 1s 16ms/step - loss: 0.1207 - accuracy: 0.9649 - val_loss: 0.1159 - val_accuracy: 0.9651\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.1210 - accuracy: 0.9656 - val_loss: 0.1158 - val_accuracy: 0.9653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2732f3a5d00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 시 loss, accuracy, 레이어별 활성화 함수의 출력값 등을 시각화시켜줌\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensor_board = TensorBoard(log_dir='tensor_log', embeddings_freq=1, histogram_freq=1)\n",
    "\n",
    "model.fit(train_X, train_y, validation_data=(test_X, test_y), batch_size=1000,\n",
    "          epochs=100, callbacks=[tensor_board], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist dataset dnn training\n",
    "# using checkpoint, early stopping, tensorboard callback\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_X, train_y),(test_X, test_y) = mnist.load_data()\n",
    "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "path = 'mnist_keras_model.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=path, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping=EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensor_board = TensorBoard(log_dir=\"tensor_log\", embeddings_freq=1, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 2.1655 - accuracy: 0.2490\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.54600, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 2s 23ms/step - loss: 2.1500 - accuracy: 0.2662 - val_loss: 1.9479 - val_accuracy: 0.5460\n",
      "Epoch 2/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 1.8008 - accuracy: 0.5463\n",
      "Epoch 00002: val_accuracy improved from 0.54600 to 0.67858, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 1.7975 - accuracy: 0.5476 - val_loss: 1.5937 - val_accuracy: 0.6786\n",
      "Epoch 3/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.4784 - accuracy: 0.6454\n",
      "Epoch 00003: val_accuracy improved from 0.67858 to 0.74325, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 1.4784 - accuracy: 0.6454 - val_loss: 1.2810 - val_accuracy: 0.7433\n",
      "Epoch 4/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 1.2240 - accuracy: 0.7038\n",
      "Epoch 00004: val_accuracy improved from 0.74325 to 0.78967, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 1.2192 - accuracy: 0.7047 - val_loss: 1.0437 - val_accuracy: 0.7897\n",
      "Epoch 5/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 1.0340 - accuracy: 0.7420\n",
      "Epoch 00005: val_accuracy improved from 0.78967 to 0.82275, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 1.0308 - accuracy: 0.7426 - val_loss: 0.8770 - val_accuracy: 0.8227\n",
      "Epoch 6/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.9021 - accuracy: 0.7712\n",
      "Epoch 00006: val_accuracy improved from 0.82275 to 0.84225, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.9015 - accuracy: 0.7712 - val_loss: 0.7617 - val_accuracy: 0.8422\n",
      "Epoch 7/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.8104 - accuracy: 0.7902\n",
      "Epoch 00007: val_accuracy improved from 0.84225 to 0.85408, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.8081 - accuracy: 0.7911 - val_loss: 0.6793 - val_accuracy: 0.8541\n",
      "Epoch 8/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.7432 - accuracy: 0.8039\n",
      "Epoch 00008: val_accuracy improved from 0.85408 to 0.86283, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.7418 - accuracy: 0.8043 - val_loss: 0.6188 - val_accuracy: 0.8628\n",
      "Epoch 9/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.6874 - accuracy: 0.8142\n",
      "Epoch 00009: val_accuracy improved from 0.86283 to 0.87075, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.6869 - accuracy: 0.8144 - val_loss: 0.5718 - val_accuracy: 0.8708\n",
      "Epoch 10/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6442 - accuracy: 0.8250\n",
      "Epoch 00010: val_accuracy improved from 0.87075 to 0.87500, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6442 - accuracy: 0.8250 - val_loss: 0.5349 - val_accuracy: 0.8750\n",
      "Epoch 11/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.8353\n",
      "Epoch 00011: val_accuracy improved from 0.87500 to 0.87958, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6089 - accuracy: 0.8353 - val_loss: 0.5046 - val_accuracy: 0.8796\n",
      "Epoch 12/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.5804 - accuracy: 0.8413\n",
      "Epoch 00012: val_accuracy improved from 0.87958 to 0.88250, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.5801 - accuracy: 0.8416 - val_loss: 0.4799 - val_accuracy: 0.8825\n",
      "Epoch 13/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.5589 - accuracy: 0.8463\n",
      "Epoch 00013: val_accuracy improved from 0.88250 to 0.88658, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.5586 - accuracy: 0.8465 - val_loss: 0.4596 - val_accuracy: 0.8866\n",
      "Epoch 14/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.5362 - accuracy: 0.8538\n",
      "Epoch 00014: val_accuracy improved from 0.88658 to 0.88967, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.5366 - accuracy: 0.8538 - val_loss: 0.4415 - val_accuracy: 0.8897\n",
      "Epoch 15/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.5150 - accuracy: 0.8582\n",
      "Epoch 00015: val_accuracy improved from 0.88967 to 0.89217, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.5158 - accuracy: 0.8579 - val_loss: 0.4264 - val_accuracy: 0.8922\n",
      "Epoch 16/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.8590\n",
      "Epoch 00016: val_accuracy improved from 0.89217 to 0.89417, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.5037 - accuracy: 0.8590 - val_loss: 0.4130 - val_accuracy: 0.8942\n",
      "Epoch 17/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.4856 - accuracy: 0.8636\n",
      "Epoch 00017: val_accuracy improved from 0.89417 to 0.89608, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.4852 - accuracy: 0.8636 - val_loss: 0.4011 - val_accuracy: 0.8961\n",
      "Epoch 18/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.4751 - accuracy: 0.8672\n",
      "Epoch 00018: val_accuracy improved from 0.89608 to 0.89958, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.4759 - accuracy: 0.8669 - val_loss: 0.3909 - val_accuracy: 0.8996\n",
      "Epoch 19/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.8687\n",
      "Epoch 00019: val_accuracy improved from 0.89958 to 0.90133, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.4631 - accuracy: 0.8687 - val_loss: 0.3819 - val_accuracy: 0.9013\n",
      "Epoch 20/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.4544 - accuracy: 0.8724\n",
      "Epoch 00020: val_accuracy improved from 0.90133 to 0.90358, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.4542 - accuracy: 0.8725 - val_loss: 0.3734 - val_accuracy: 0.9036\n",
      "Epoch 21/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.4450 - accuracy: 0.8748\n",
      "Epoch 00021: val_accuracy improved from 0.90358 to 0.90500, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.4449 - accuracy: 0.8746 - val_loss: 0.3659 - val_accuracy: 0.9050\n",
      "Epoch 22/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.8776\n",
      "Epoch 00022: val_accuracy improved from 0.90500 to 0.90608, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.4361 - accuracy: 0.8776 - val_loss: 0.3593 - val_accuracy: 0.9061\n",
      "Epoch 23/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8788\n",
      "Epoch 00023: val_accuracy improved from 0.90608 to 0.90742, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.4282 - accuracy: 0.8788 - val_loss: 0.3527 - val_accuracy: 0.9074\n",
      "Epoch 24/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.8816\n",
      "Epoch 00024: val_accuracy improved from 0.90742 to 0.90800, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.4187 - accuracy: 0.8816 - val_loss: 0.3471 - val_accuracy: 0.9080\n",
      "Epoch 25/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.4149 - accuracy: 0.8826\n",
      "Epoch 00025: val_accuracy improved from 0.90800 to 0.90983, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.4144 - accuracy: 0.8827 - val_loss: 0.3417 - val_accuracy: 0.9098\n",
      "Epoch 26/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.4055 - accuracy: 0.8845\n",
      "Epoch 00026: val_accuracy improved from 0.90983 to 0.91050, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.4059 - accuracy: 0.8843 - val_loss: 0.3366 - val_accuracy: 0.9105\n",
      "Epoch 27/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.8854\n",
      "Epoch 00027: val_accuracy improved from 0.91050 to 0.91142, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.4015 - accuracy: 0.8854 - val_loss: 0.3320 - val_accuracy: 0.9114\n",
      "Epoch 28/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3968 - accuracy: 0.8890\n",
      "Epoch 00028: val_accuracy improved from 0.91142 to 0.91250, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3969 - accuracy: 0.8890 - val_loss: 0.3277 - val_accuracy: 0.9125\n",
      "Epoch 29/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3897 - accuracy: 0.8895\n",
      "Epoch 00029: val_accuracy improved from 0.91250 to 0.91308, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3910 - accuracy: 0.8888 - val_loss: 0.3236 - val_accuracy: 0.9131\n",
      "Epoch 30/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3851 - accuracy: 0.8912\n",
      "Epoch 00030: val_accuracy improved from 0.91308 to 0.91375, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3850 - accuracy: 0.8912 - val_loss: 0.3198 - val_accuracy: 0.9137\n",
      "Epoch 31/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3816 - accuracy: 0.8919\n",
      "Epoch 00031: val_accuracy improved from 0.91375 to 0.91408, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3815 - accuracy: 0.8916 - val_loss: 0.3162 - val_accuracy: 0.9141\n",
      "Epoch 32/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3753 - accuracy: 0.8946\n",
      "Epoch 00032: val_accuracy improved from 0.91408 to 0.91450, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3763 - accuracy: 0.8943 - val_loss: 0.3126 - val_accuracy: 0.9145\n",
      "Epoch 33/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3735 - accuracy: 0.8940\n",
      "Epoch 00033: val_accuracy improved from 0.91450 to 0.91550, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3730 - accuracy: 0.8941 - val_loss: 0.3093 - val_accuracy: 0.9155\n",
      "Epoch 34/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3677 - accuracy: 0.8953\n",
      "Epoch 00034: val_accuracy did not improve from 0.91550\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3676 - accuracy: 0.8952 - val_loss: 0.3061 - val_accuracy: 0.9152\n",
      "Epoch 35/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8963\n",
      "Epoch 00035: val_accuracy improved from 0.91550 to 0.91700, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3636 - accuracy: 0.8964 - val_loss: 0.3033 - val_accuracy: 0.9170\n",
      "Epoch 36/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8982 ETA: 0s - loss: 0.3608 - \n",
      "Epoch 00036: val_accuracy improved from 0.91700 to 0.91750, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.3576 - accuracy: 0.8981 - val_loss: 0.3001 - val_accuracy: 0.9175\n",
      "Epoch 37/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.3539 - accuracy: 0.8995\n",
      "Epoch 00037: val_accuracy improved from 0.91750 to 0.91775, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3549 - accuracy: 0.8990 - val_loss: 0.2974 - val_accuracy: 0.9178\n",
      "Epoch 38/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.3533 - accuracy: 0.8988\n",
      "Epoch 00038: val_accuracy improved from 0.91775 to 0.91867, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3533 - accuracy: 0.8988 - val_loss: 0.2946 - val_accuracy: 0.9187\n",
      "Epoch 39/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3503 - accuracy: 0.9020\n",
      "Epoch 00039: val_accuracy did not improve from 0.91867\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3501 - accuracy: 0.9018 - val_loss: 0.2920 - val_accuracy: 0.9187\n",
      "Epoch 40/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3454 - accuracy: 0.9013\n",
      "Epoch 00040: val_accuracy improved from 0.91867 to 0.91958, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3454 - accuracy: 0.9013 - val_loss: 0.2895 - val_accuracy: 0.9196\n",
      "Epoch 41/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9021\n",
      "Epoch 00041: val_accuracy improved from 0.91958 to 0.91992, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3434 - accuracy: 0.9020 - val_loss: 0.2871 - val_accuracy: 0.9199\n",
      "Epoch 42/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3401 - accuracy: 0.9026\n",
      "Epoch 00042: val_accuracy improved from 0.91992 to 0.92083, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.3410 - accuracy: 0.9025 - val_loss: 0.2847 - val_accuracy: 0.9208\n",
      "Epoch 43/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.3370 - accuracy: 0.9038\n",
      "Epoch 00043: val_accuracy did not improve from 0.92083\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3366 - accuracy: 0.9037 - val_loss: 0.2823 - val_accuracy: 0.9208\n",
      "Epoch 44/1000\n",
      "44/48 [==========================>...] - ETA: 0s - loss: 0.3346 - accuracy: 0.9040 ETA: 0s - loss: 0.3342 - accuracy: 0.90\n",
      "Epoch 00044: val_accuracy improved from 0.92083 to 0.92167, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3355 - accuracy: 0.9040 - val_loss: 0.2802 - val_accuracy: 0.9217\n",
      "Epoch 45/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3304 - accuracy: 0.9046\n",
      "Epoch 00045: val_accuracy improved from 0.92167 to 0.92267, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.3313 - accuracy: 0.9044 - val_loss: 0.2780 - val_accuracy: 0.9227\n",
      "Epoch 46/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.3265 - accuracy: 0.9053\n",
      "Epoch 00046: val_accuracy improved from 0.92267 to 0.92317, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3277 - accuracy: 0.9054 - val_loss: 0.2759 - val_accuracy: 0.9232\n",
      "Epoch 47/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.9071\n",
      "Epoch 00047: val_accuracy improved from 0.92317 to 0.92392, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3260 - accuracy: 0.9073 - val_loss: 0.2738 - val_accuracy: 0.9239\n",
      "Epoch 48/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3260 - accuracy: 0.9059\n",
      "Epoch 00048: val_accuracy improved from 0.92392 to 0.92400, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3253 - accuracy: 0.9063 - val_loss: 0.2721 - val_accuracy: 0.9240\n",
      "Epoch 49/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.3206 - accuracy: 0.9090\n",
      "Epoch 00049: val_accuracy improved from 0.92400 to 0.92475, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3207 - accuracy: 0.9088 - val_loss: 0.2700 - val_accuracy: 0.9247\n",
      "Epoch 50/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3185 - accuracy: 0.9091\n",
      "Epoch 00050: val_accuracy improved from 0.92475 to 0.92550, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3193 - accuracy: 0.9090 - val_loss: 0.2680 - val_accuracy: 0.9255\n",
      "Epoch 51/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3160 - accuracy: 0.9107\n",
      "Epoch 00051: val_accuracy improved from 0.92550 to 0.92592, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3160 - accuracy: 0.9106 - val_loss: 0.2662 - val_accuracy: 0.9259\n",
      "Epoch 52/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3136 - accuracy: 0.9116\n",
      "Epoch 00052: val_accuracy improved from 0.92592 to 0.92633, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3140 - accuracy: 0.9115 - val_loss: 0.2643 - val_accuracy: 0.9263\n",
      "Epoch 53/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.3118 - accuracy: 0.9093\n",
      "Epoch 00053: val_accuracy improved from 0.92633 to 0.92733, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3107 - accuracy: 0.9099 - val_loss: 0.2625 - val_accuracy: 0.9273\n",
      "Epoch 54/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.9115\n",
      "Epoch 00054: val_accuracy improved from 0.92733 to 0.92742, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3083 - accuracy: 0.9119 - val_loss: 0.2607 - val_accuracy: 0.9274\n",
      "Epoch 55/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.3067 - accuracy: 0.9119\n",
      "Epoch 00055: val_accuracy improved from 0.92742 to 0.92775, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3076 - accuracy: 0.9120 - val_loss: 0.2589 - val_accuracy: 0.9277\n",
      "Epoch 56/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.9136\n",
      "Epoch 00056: val_accuracy improved from 0.92775 to 0.92825, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.3031 - accuracy: 0.9137 - val_loss: 0.2573 - val_accuracy: 0.9283\n",
      "Epoch 57/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.3025 - accuracy: 0.9139\n",
      "Epoch 00057: val_accuracy improved from 0.92825 to 0.92908, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3033 - accuracy: 0.9135 - val_loss: 0.2556 - val_accuracy: 0.9291\n",
      "Epoch 58/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.9131\n",
      "Epoch 00058: val_accuracy improved from 0.92908 to 0.92917, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.3009 - accuracy: 0.9129 - val_loss: 0.2540 - val_accuracy: 0.9292\n",
      "Epoch 59/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2983 - accuracy: 0.9139\n",
      "Epoch 00059: val_accuracy improved from 0.92917 to 0.92967, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2991 - accuracy: 0.9138 - val_loss: 0.2525 - val_accuracy: 0.9297\n",
      "Epoch 60/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.9143\n",
      "Epoch 00060: val_accuracy improved from 0.92967 to 0.93033, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2972 - accuracy: 0.9143 - val_loss: 0.2511 - val_accuracy: 0.9303\n",
      "Epoch 61/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2939 - accuracy: 0.9152\n",
      "Epoch 00061: val_accuracy improved from 0.93033 to 0.93058, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2937 - accuracy: 0.9157 - val_loss: 0.2494 - val_accuracy: 0.9306\n",
      "Epoch 62/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.9176\n",
      "Epoch 00062: val_accuracy did not improve from 0.93058\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2936 - accuracy: 0.9175 - val_loss: 0.2480 - val_accuracy: 0.9305\n",
      "Epoch 63/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2921 - accuracy: 0.9171\n",
      "Epoch 00063: val_accuracy improved from 0.93058 to 0.93108, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2916 - accuracy: 0.9173 - val_loss: 0.2466 - val_accuracy: 0.9311\n",
      "Epoch 64/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2895 - accuracy: 0.9163\n",
      "Epoch 00064: val_accuracy improved from 0.93108 to 0.93150, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2904 - accuracy: 0.9159 - val_loss: 0.2452 - val_accuracy: 0.9315\n",
      "Epoch 65/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9179\n",
      "Epoch 00065: val_accuracy improved from 0.93150 to 0.93167, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2883 - accuracy: 0.9179 - val_loss: 0.2437 - val_accuracy: 0.9317\n",
      "Epoch 66/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9189\n",
      "Epoch 00066: val_accuracy improved from 0.93167 to 0.93192, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2859 - accuracy: 0.9187 - val_loss: 0.2424 - val_accuracy: 0.9319\n",
      "Epoch 67/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2834 - accuracy: 0.9194\n",
      "Epoch 00067: val_accuracy improved from 0.93192 to 0.93250, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2827 - accuracy: 0.9196 - val_loss: 0.2409 - val_accuracy: 0.9325\n",
      "Epoch 68/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.9202\n",
      "Epoch 00068: val_accuracy improved from 0.93250 to 0.93283, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2821 - accuracy: 0.9202 - val_loss: 0.2397 - val_accuracy: 0.9328\n",
      "Epoch 69/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2792 - accuracy: 0.9196\n",
      "Epoch 00069: val_accuracy improved from 0.93283 to 0.93292, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2798 - accuracy: 0.9195 - val_loss: 0.2384 - val_accuracy: 0.9329\n",
      "Epoch 70/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2789 - accuracy: 0.9199\n",
      "Epoch 00070: val_accuracy improved from 0.93292 to 0.93375, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2791 - accuracy: 0.9198 - val_loss: 0.2369 - val_accuracy: 0.9337\n",
      "Epoch 71/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.9208\n",
      "Epoch 00071: val_accuracy improved from 0.93375 to 0.93408, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2767 - accuracy: 0.9205 - val_loss: 0.2358 - val_accuracy: 0.9341\n",
      "Epoch 72/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2758 - accuracy: 0.9216\n",
      "Epoch 00072: val_accuracy improved from 0.93408 to 0.93417, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2755 - accuracy: 0.9217 - val_loss: 0.2344 - val_accuracy: 0.9342\n",
      "Epoch 73/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2752 - accuracy: 0.9221\n",
      "Epoch 00073: val_accuracy improved from 0.93417 to 0.93483, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2740 - accuracy: 0.9225 - val_loss: 0.2333 - val_accuracy: 0.9348\n",
      "Epoch 74/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2721 - accuracy: 0.9216\n",
      "Epoch 00074: val_accuracy did not improve from 0.93483\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2714 - accuracy: 0.9219 - val_loss: 0.2322 - val_accuracy: 0.9342\n",
      "Epoch 75/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.9229\n",
      "Epoch 00075: val_accuracy improved from 0.93483 to 0.93542, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2710 - accuracy: 0.9229 - val_loss: 0.2310 - val_accuracy: 0.9354\n",
      "Epoch 76/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2680 - accuracy: 0.9233\n",
      "Epoch 00076: val_accuracy improved from 0.93542 to 0.93608, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2675 - accuracy: 0.9233 - val_loss: 0.2296 - val_accuracy: 0.9361\n",
      "Epoch 77/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2664 - accuracy: 0.9254\n",
      "Epoch 00077: val_accuracy improved from 0.93608 to 0.93625, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2676 - accuracy: 0.9253 - val_loss: 0.2285 - val_accuracy: 0.9362\n",
      "Epoch 78/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2660 - accuracy: 0.9235\n",
      "Epoch 00078: val_accuracy improved from 0.93625 to 0.93683, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2655 - accuracy: 0.9235 - val_loss: 0.2275 - val_accuracy: 0.9368\n",
      "Epoch 79/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2648 - accuracy: 0.9242\n",
      "Epoch 00079: val_accuracy did not improve from 0.93683\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2646 - accuracy: 0.9243 - val_loss: 0.2262 - val_accuracy: 0.9367\n",
      "Epoch 80/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2637 - accuracy: 0.9246\n",
      "Epoch 00080: val_accuracy improved from 0.93683 to 0.93775, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2633 - accuracy: 0.9247 - val_loss: 0.2252 - val_accuracy: 0.9377\n",
      "Epoch 81/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9252\n",
      "Epoch 00081: val_accuracy did not improve from 0.93775\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2623 - accuracy: 0.9252 - val_loss: 0.2242 - val_accuracy: 0.9377\n",
      "Epoch 82/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9261\n",
      "Epoch 00082: val_accuracy improved from 0.93775 to 0.93800, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2602 - accuracy: 0.9261 - val_loss: 0.2233 - val_accuracy: 0.9380\n",
      "Epoch 83/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9266\n",
      "Epoch 00083: val_accuracy improved from 0.93800 to 0.93817, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2586 - accuracy: 0.9266 - val_loss: 0.2220 - val_accuracy: 0.9382\n",
      "Epoch 84/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9273\n",
      "Epoch 00084: val_accuracy improved from 0.93817 to 0.93850, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2580 - accuracy: 0.9273 - val_loss: 0.2210 - val_accuracy: 0.9385\n",
      "Epoch 85/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2560 - accuracy: 0.9261\n",
      "Epoch 00085: val_accuracy improved from 0.93850 to 0.93875, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2556 - accuracy: 0.9260 - val_loss: 0.2201 - val_accuracy: 0.9388\n",
      "Epoch 86/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2575 - accuracy: 0.9268\n",
      "Epoch 00086: val_accuracy improved from 0.93875 to 0.93900, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2575 - accuracy: 0.9266 - val_loss: 0.2191 - val_accuracy: 0.9390\n",
      "Epoch 87/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.9273\n",
      "Epoch 00087: val_accuracy improved from 0.93900 to 0.93958, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2547 - accuracy: 0.9273 - val_loss: 0.2178 - val_accuracy: 0.9396\n",
      "Epoch 88/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2537 - accuracy: 0.9266\n",
      "Epoch 00088: val_accuracy improved from 0.93958 to 0.93967, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2529 - accuracy: 0.9270 - val_loss: 0.2170 - val_accuracy: 0.9397\n",
      "Epoch 89/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9292\n",
      "Epoch 00089: val_accuracy did not improve from 0.93967\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.2497 - accuracy: 0.9290 - val_loss: 0.2160 - val_accuracy: 0.9395\n",
      "Epoch 90/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9295\n",
      "Epoch 00090: val_accuracy improved from 0.93967 to 0.93975, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2499 - accuracy: 0.9295 - val_loss: 0.2149 - val_accuracy: 0.9398\n",
      "Epoch 91/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2488 - accuracy: 0.9286\n",
      "Epoch 00091: val_accuracy improved from 0.93975 to 0.94050, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2490 - accuracy: 0.9285 - val_loss: 0.2139 - val_accuracy: 0.9405\n",
      "Epoch 92/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.9301\n",
      "Epoch 00092: val_accuracy improved from 0.94050 to 0.94092, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2450 - accuracy: 0.9301 - val_loss: 0.2130 - val_accuracy: 0.9409\n",
      "Epoch 93/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2459 - accuracy: 0.9302\n",
      "Epoch 00093: val_accuracy did not improve from 0.94092\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2461 - accuracy: 0.9299 - val_loss: 0.2120 - val_accuracy: 0.9406\n",
      "Epoch 94/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2472 - accuracy: 0.9300\n",
      "Epoch 00094: val_accuracy improved from 0.94092 to 0.94117, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2449 - accuracy: 0.9306 - val_loss: 0.2110 - val_accuracy: 0.9412\n",
      "Epoch 95/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9307\n",
      "Epoch 00095: val_accuracy did not improve from 0.94117\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2425 - accuracy: 0.9304 - val_loss: 0.2101 - val_accuracy: 0.9412\n",
      "Epoch 96/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2428 - accuracy: 0.9304\n",
      "Epoch 00096: val_accuracy improved from 0.94117 to 0.94200, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2422 - accuracy: 0.9306 - val_loss: 0.2092 - val_accuracy: 0.9420\n",
      "Epoch 97/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.9313\n",
      "Epoch 00097: val_accuracy improved from 0.94200 to 0.94242, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2415 - accuracy: 0.9312 - val_loss: 0.2083 - val_accuracy: 0.9424\n",
      "Epoch 98/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9307\n",
      "Epoch 00098: val_accuracy improved from 0.94242 to 0.94267, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2406 - accuracy: 0.9307 - val_loss: 0.2074 - val_accuracy: 0.9427\n",
      "Epoch 99/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2377 - accuracy: 0.9317\n",
      "Epoch 00099: val_accuracy did not improve from 0.94267\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2377 - accuracy: 0.9317 - val_loss: 0.2064 - val_accuracy: 0.9427\n",
      "Epoch 100/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2387 - accuracy: 0.9320\n",
      "Epoch 00100: val_accuracy did not improve from 0.94267\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2386 - accuracy: 0.9317 - val_loss: 0.2056 - val_accuracy: 0.9427\n",
      "Epoch 101/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.9323\n",
      "Epoch 00101: val_accuracy improved from 0.94267 to 0.94325, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.2366 - accuracy: 0.9323 - val_loss: 0.2048 - val_accuracy: 0.9433\n",
      "Epoch 102/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2362 - accuracy: 0.9324\n",
      "Epoch 00102: val_accuracy did not improve from 0.94325\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2360 - accuracy: 0.9325 - val_loss: 0.2041 - val_accuracy: 0.9433\n",
      "Epoch 103/1000\n",
      "45/48 [===========================>..] - ETA: 0s - loss: 0.2342 - accuracy: 0.9330\n",
      "Epoch 00103: val_accuracy improved from 0.94325 to 0.94333, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.2343 - accuracy: 0.9329 - val_loss: 0.2032 - val_accuracy: 0.9433\n",
      "Epoch 104/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9339\n",
      "Epoch 00104: val_accuracy improved from 0.94333 to 0.94358, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2333 - accuracy: 0.9339 - val_loss: 0.2021 - val_accuracy: 0.9436\n",
      "Epoch 105/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2345 - accuracy: 0.9330\n",
      "Epoch 00105: val_accuracy improved from 0.94358 to 0.94383, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2340 - accuracy: 0.9331 - val_loss: 0.2015 - val_accuracy: 0.9438\n",
      "Epoch 106/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9336\n",
      "Epoch 00106: val_accuracy improved from 0.94383 to 0.94417, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.2302 - accuracy: 0.9336 - val_loss: 0.2006 - val_accuracy: 0.9442\n",
      "Epoch 107/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2301 - accuracy: 0.9339\n",
      "Epoch 00107: val_accuracy improved from 0.94417 to 0.94467, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.2307 - accuracy: 0.9337 - val_loss: 0.1998 - val_accuracy: 0.9447\n",
      "Epoch 108/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9340\n",
      "Epoch 00108: val_accuracy improved from 0.94467 to 0.94492, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.2284 - accuracy: 0.9340 - val_loss: 0.1991 - val_accuracy: 0.9449\n",
      "Epoch 109/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.9340\n",
      "Epoch 00109: val_accuracy did not improve from 0.94492\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2268 - accuracy: 0.9342 - val_loss: 0.1982 - val_accuracy: 0.9449\n",
      "Epoch 110/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.9348 ETA: 0s - loss: 0\n",
      "Epoch 00110: val_accuracy improved from 0.94492 to 0.94517, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.2266 - accuracy: 0.9348 - val_loss: 0.1974 - val_accuracy: 0.9452\n",
      "Epoch 111/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2254 - accuracy: 0.9347\n",
      "Epoch 00111: val_accuracy improved from 0.94517 to 0.94558, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.2254 - accuracy: 0.9350 - val_loss: 0.1965 - val_accuracy: 0.9456\n",
      "Epoch 112/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2250 - accuracy: 0.9356\n",
      "Epoch 00112: val_accuracy improved from 0.94558 to 0.94567, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2257 - accuracy: 0.9353 - val_loss: 0.1958 - val_accuracy: 0.9457\n",
      "Epoch 113/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9354\n",
      "Epoch 00113: val_accuracy improved from 0.94567 to 0.94608, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2254 - accuracy: 0.9354 - val_loss: 0.1950 - val_accuracy: 0.9461\n",
      "Epoch 114/1000\n",
      "46/48 [===========================>..] - ETA: 0s - loss: 0.2232 - accuracy: 0.9357\n",
      "Epoch 00114: val_accuracy did not improve from 0.94608\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2226 - accuracy: 0.9361 - val_loss: 0.1944 - val_accuracy: 0.9460\n",
      "Epoch 115/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9362\n",
      "Epoch 00115: val_accuracy did not improve from 0.94608\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2236 - accuracy: 0.9362 - val_loss: 0.1936 - val_accuracy: 0.9457\n",
      "Epoch 116/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 0.9367\n",
      "Epoch 00116: val_accuracy improved from 0.94608 to 0.94683, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2225 - accuracy: 0.9367 - val_loss: 0.1929 - val_accuracy: 0.9468\n",
      "Epoch 117/1000\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9359\n",
      "Epoch 00117: val_accuracy did not improve from 0.94683\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2204 - accuracy: 0.9359 - val_loss: 0.1922 - val_accuracy: 0.9463\n",
      "Epoch 118/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2183 - accuracy: 0.9381\n",
      "Epoch 00118: val_accuracy improved from 0.94683 to 0.94733, saving model to mnist_keras_model.h5\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 0.2184 - accuracy: 0.9379 - val_loss: 0.1913 - val_accuracy: 0.9473\n",
      "Epoch 119/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2189 - accuracy: 0.9372\n",
      "Epoch 00119: val_accuracy did not improve from 0.94733\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.2190 - accuracy: 0.9373 - val_loss: 0.1907 - val_accuracy: 0.9468\n",
      "Epoch 120/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2180 - accuracy: 0.9382\n",
      "Epoch 00120: val_accuracy did not improve from 0.94733\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2178 - accuracy: 0.9381 - val_loss: 0.1900 - val_accuracy: 0.9473\n",
      "Epoch 121/1000\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2143 - accuracy: 0.9385\n",
      "Epoch 00121: val_accuracy did not improve from 0.94733\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2146 - accuracy: 0.9384 - val_loss: 0.1892 - val_accuracy: 0.9470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15e1d8e0e20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(360, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_X, train_y, validation_split=0.2, batch_size=1000, epochs=1000,\n",
    "          callbacks=[checkpoint, early_stopping, tensor_board])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 케라스를 이용한 와인데이터 분류용 DNN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 와인 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 케라스를 이용한 DNN 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    681\n",
       "3    638\n",
       "4    199\n",
       "1     53\n",
       "5     18\n",
       "0     10\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "redwine = pd.read_csv('winequality-red.csv', sep=';')\n",
    "redwine_X = redwine.iloc[:, :-1]\n",
    "redwine_y = redwine.iloc[:, -1] - 3\n",
    "redwine_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 7ms/step - loss: 1.6473 - accuracy: 0.3021\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.2787 - accuracy: 0.4316\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.2597 - accuracy: 0.4155\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.2358 - accuracy: 0.4406\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.2121 - accuracy: 0.4415\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1990 - accuracy: 0.4692\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2050 - accuracy: 0.4352\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1999 - accuracy: 0.4656\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1778 - accuracy: 0.4745\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1634 - accuracy: 0.4656\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1530 - accuracy: 0.4710\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1657 - accuracy: 0.4808\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1682 - accuracy: 0.4870\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1448 - accuracy: 0.4772\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1459 - accuracy: 0.4781\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1618 - accuracy: 0.4817\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1496 - accuracy: 0.4718\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1533 - accuracy: 0.4674\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1358 - accuracy: 0.4933\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1552 - accuracy: 0.4727\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1371 - accuracy: 0.4924\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1396 - accuracy: 0.4906\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1343 - accuracy: 0.4942\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1488 - accuracy: 0.4879\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1321 - accuracy: 0.4915\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1300 - accuracy: 0.4987\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1257 - accuracy: 0.4835\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1192 - accuracy: 0.5103\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1323 - accuracy: 0.5022\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1331 - accuracy: 0.4978\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1195 - accuracy: 0.5139\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1225 - accuracy: 0.5156\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1190 - accuracy: 0.5103\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1065 - accuracy: 0.5228\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1102 - accuracy: 0.5282\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1172 - accuracy: 0.5156\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.1221 - accuracy: 0.4969\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1076 - accuracy: 0.5139\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1022 - accuracy: 0.5246\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0980 - accuracy: 0.5094\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1097 - accuracy: 0.5156\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1045 - accuracy: 0.5112\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0938 - accuracy: 0.5183\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0808 - accuracy: 0.5103\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0884 - accuracy: 0.5121\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.0982 - accuracy: 0.5130\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0851 - accuracy: 0.5076\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0892 - accuracy: 0.5246\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0792 - accuracy: 0.5139\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0736 - accuracy: 0.5523\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0828 - accuracy: 0.5290\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0784 - accuracy: 0.5264\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0789 - accuracy: 0.5308\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0691 - accuracy: 0.5282\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0690 - accuracy: 0.5290\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0848 - accuracy: 0.5201\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0856 - accuracy: 0.5246\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0653 - accuracy: 0.5442\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0672 - accuracy: 0.5344\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0654 - accuracy: 0.5308\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0638 - accuracy: 0.5174\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0711 - accuracy: 0.5264\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0464 - accuracy: 0.5505\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0622 - accuracy: 0.5550\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0586 - accuracy: 0.5308\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0457 - accuracy: 0.5371\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0539 - accuracy: 0.5299\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0381 - accuracy: 0.5514\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0424 - accuracy: 0.5416\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0535 - accuracy: 0.5389\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0415 - accuracy: 0.5505\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0569 - accuracy: 0.5326\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0511 - accuracy: 0.5398\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0359 - accuracy: 0.5559\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0421 - accuracy: 0.5416\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.0270 - accuracy: 0.5567\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0458 - accuracy: 0.5353\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0480 - accuracy: 0.5433\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0380 - accuracy: 0.5362\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0315 - accuracy: 0.5317\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0279 - accuracy: 0.5460\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0319 - accuracy: 0.5523\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0347 - accuracy: 0.5496\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0338 - accuracy: 0.5442\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0261 - accuracy: 0.5523\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0309 - accuracy: 0.5496\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0216 - accuracy: 0.5559\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0316 - accuracy: 0.5559\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0230 - accuracy: 0.5567\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0221 - accuracy: 0.5576\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0111 - accuracy: 0.5648\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0163 - accuracy: 0.5585\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0174 - accuracy: 0.5451\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0151 - accuracy: 0.5550\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0104 - accuracy: 0.5657\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0094 - accuracy: 0.5630\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0182 - accuracy: 0.5639\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0277 - accuracy: 0.5487\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0202 - accuracy: 0.5442\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0269 - accuracy: 0.5505\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0149 - accuracy: 0.5532\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0149 - accuracy: 0.5424\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0040 - accuracy: 0.5594\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9994 - accuracy: 0.5567\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0222 - accuracy: 0.5407\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0156 - accuracy: 0.5585\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0014 - accuracy: 0.5639\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0056 - accuracy: 0.5585\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0059 - accuracy: 0.5737\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0119 - accuracy: 0.5576\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0035 - accuracy: 0.5621\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.0059 - accuracy: 0.5612\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0005 - accuracy: 0.5603\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0196 - accuracy: 0.5451\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9937 - accuracy: 0.5657\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0017 - accuracy: 0.5630\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9881 - accuracy: 0.5702\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0031 - accuracy: 0.5576\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9965 - accuracy: 0.5657\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9983 - accuracy: 0.5523\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9898 - accuracy: 0.5612\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9952 - accuracy: 0.5719\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9884 - accuracy: 0.5496\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0019 - accuracy: 0.5693\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9945 - accuracy: 0.5657\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9856 - accuracy: 0.5710\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9957 - accuracy: 0.5505\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0118 - accuracy: 0.5523\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0075 - accuracy: 0.5630\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9977 - accuracy: 0.5559\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0045 - accuracy: 0.5496\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.0023 - accuracy: 0.5737\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9904 - accuracy: 0.5657\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9987 - accuracy: 0.5594\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9892 - accuracy: 0.5603\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9811 - accuracy: 0.5630\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9793 - accuracy: 0.5693\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9847 - accuracy: 0.5541\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9946 - accuracy: 0.5630\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9833 - accuracy: 0.5773\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9721 - accuracy: 0.5612\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9811 - accuracy: 0.5836\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9766 - accuracy: 0.5728\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9743 - accuracy: 0.5746\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9842 - accuracy: 0.5728\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9765 - accuracy: 0.5827\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9894 - accuracy: 0.5514\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9840 - accuracy: 0.5710\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9670 - accuracy: 0.5782\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9691 - accuracy: 0.5880\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9870 - accuracy: 0.5746\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9894 - accuracy: 0.5675\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9862 - accuracy: 0.5693\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9798 - accuracy: 0.5666\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9822 - accuracy: 0.5675\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9730 - accuracy: 0.5702\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9828 - accuracy: 0.5541\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9843 - accuracy: 0.5702\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9736 - accuracy: 0.5541\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9755 - accuracy: 0.5836\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9801 - accuracy: 0.5791\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9791 - accuracy: 0.5737\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9784 - accuracy: 0.5827\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9837 - accuracy: 0.5702\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9813 - accuracy: 0.5585\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9764 - accuracy: 0.5737\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9602 - accuracy: 0.5755\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9745 - accuracy: 0.5657\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9831 - accuracy: 0.5809\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9827 - accuracy: 0.5675\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9487 - accuracy: 0.5800\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9725 - accuracy: 0.5684\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9654 - accuracy: 0.5782\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9576 - accuracy: 0.5862\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9707 - accuracy: 0.5514\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9780 - accuracy: 0.5746\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9562 - accuracy: 0.5862\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9677 - accuracy: 0.5782\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9754 - accuracy: 0.5666\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9699 - accuracy: 0.5746\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9552 - accuracy: 0.5871\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9556 - accuracy: 0.5871\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9831 - accuracy: 0.5755\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9549 - accuracy: 0.5755\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9585 - accuracy: 0.5827\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9447 - accuracy: 0.6005\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9571 - accuracy: 0.5782\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9654 - accuracy: 0.5791\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9646 - accuracy: 0.5773\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9691 - accuracy: 0.5684\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9651 - accuracy: 0.5764\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9645 - accuracy: 0.5773\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9607 - accuracy: 0.5871\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9609 - accuracy: 0.5791\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9717 - accuracy: 0.5567\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9464 - accuracy: 0.5702\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9501 - accuracy: 0.5952\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9491 - accuracy: 0.5925\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9561 - accuracy: 0.5836\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9474 - accuracy: 0.5800\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9482 - accuracy: 0.5737\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9617 - accuracy: 0.5853\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9563 - accuracy: 0.5979\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9539 - accuracy: 0.5934\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9536 - accuracy: 0.5862\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9579 - accuracy: 0.5925\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9564 - accuracy: 0.5916\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9533 - accuracy: 0.5800\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9586 - accuracy: 0.5657\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9393 - accuracy: 0.5871\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9570 - accuracy: 0.5782\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9453 - accuracy: 0.5836\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9425 - accuracy: 0.5818\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9526 - accuracy: 0.5737\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9610 - accuracy: 0.5791\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9797 - accuracy: 0.5737\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9522 - accuracy: 0.5728\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9590 - accuracy: 0.5845\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9571 - accuracy: 0.5943\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9445 - accuracy: 0.5889\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9439 - accuracy: 0.5943\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9349 - accuracy: 0.5907\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9422 - accuracy: 0.5782\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9466 - accuracy: 0.6023\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9437 - accuracy: 0.5871\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9471 - accuracy: 0.5871\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9481 - accuracy: 0.5880\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9282 - accuracy: 0.5880\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9331 - accuracy: 0.6041\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9396 - accuracy: 0.5898\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9608 - accuracy: 0.5782\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9373 - accuracy: 0.5845\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9521 - accuracy: 0.6041\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9461 - accuracy: 0.5809\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9505 - accuracy: 0.5809\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9452 - accuracy: 0.6130\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9366 - accuracy: 0.5996\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9378 - accuracy: 0.5916\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9374 - accuracy: 0.5871\n",
      "Epoch 240/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9401 - accuracy: 0.5746\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9493 - accuracy: 0.5818\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9363 - accuracy: 0.5853\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9415 - accuracy: 0.5943\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9469 - accuracy: 0.5800\n",
      "Epoch 245/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9437 - accuracy: 0.6005\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9419 - accuracy: 0.5755\n",
      "Epoch 247/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9439 - accuracy: 0.5862\n",
      "Epoch 248/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9432 - accuracy: 0.5916\n",
      "Epoch 249/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9531 - accuracy: 0.5916\n",
      "Epoch 250/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9538 - accuracy: 0.5746\n",
      "Epoch 251/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9335 - accuracy: 0.5934\n",
      "Epoch 252/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9278 - accuracy: 0.6023\n",
      "Epoch 253/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9447 - accuracy: 0.5880\n",
      "Epoch 254/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9417 - accuracy: 0.5862\n",
      "Epoch 255/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9566 - accuracy: 0.5880\n",
      "Epoch 256/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9516 - accuracy: 0.5916\n",
      "Epoch 257/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9376 - accuracy: 0.6023\n",
      "Epoch 258/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9282 - accuracy: 0.5871\n",
      "Epoch 259/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9426 - accuracy: 0.5934\n",
      "Epoch 260/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9363 - accuracy: 0.6023\n",
      "Epoch 261/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9180 - accuracy: 0.6041\n",
      "Epoch 262/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9327 - accuracy: 0.5871\n",
      "Epoch 263/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9262 - accuracy: 0.6005\n",
      "Epoch 264/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9365 - accuracy: 0.5952\n",
      "Epoch 265/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9335 - accuracy: 0.5746\n",
      "Epoch 266/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9498 - accuracy: 0.5809\n",
      "Epoch 267/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9332 - accuracy: 0.5934\n",
      "Epoch 268/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9406 - accuracy: 0.5970\n",
      "Epoch 269/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9285 - accuracy: 0.5871\n",
      "Epoch 270/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9166 - accuracy: 0.6005\n",
      "Epoch 271/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9402 - accuracy: 0.5898\n",
      "Epoch 272/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9251 - accuracy: 0.5961\n",
      "Epoch 273/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9295 - accuracy: 0.5961\n",
      "Epoch 274/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9198 - accuracy: 0.6005\n",
      "Epoch 275/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9512 - accuracy: 0.5782\n",
      "Epoch 276/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9139 - accuracy: 0.5961\n",
      "Epoch 277/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9294 - accuracy: 0.5880\n",
      "Epoch 278/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9231 - accuracy: 0.5791\n",
      "Epoch 279/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9376 - accuracy: 0.5809\n",
      "Epoch 280/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9296 - accuracy: 0.5782\n",
      "Epoch 281/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9333 - accuracy: 0.5987\n",
      "Epoch 282/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9241 - accuracy: 0.5862\n",
      "Epoch 283/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9217 - accuracy: 0.6014\n",
      "Epoch 284/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9171 - accuracy: 0.5952\n",
      "Epoch 285/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9240 - accuracy: 0.5889\n",
      "Epoch 286/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9187 - accuracy: 0.6086\n",
      "Epoch 287/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9185 - accuracy: 0.5934\n",
      "Epoch 288/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9254 - accuracy: 0.5987\n",
      "Epoch 289/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9271 - accuracy: 0.6050\n",
      "Epoch 290/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9267 - accuracy: 0.5898\n",
      "Epoch 291/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9236 - accuracy: 0.5880\n",
      "Epoch 292/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9123 - accuracy: 0.5934\n",
      "Epoch 293/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9184 - accuracy: 0.6005\n",
      "Epoch 294/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9318 - accuracy: 0.5880\n",
      "Epoch 295/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9065 - accuracy: 0.6077\n",
      "Epoch 296/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9218 - accuracy: 0.5907\n",
      "Epoch 297/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9274 - accuracy: 0.5845\n",
      "Epoch 298/300\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.9455 - accuracy: 0.5898\n",
      "Epoch 299/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.9353 - accuracy: 0.5836\n",
      "Epoch 300/300\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9265 - accuracy: 0.6041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(redwine_X, redwine_y, test_size=0.3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, input_shape=(11,), activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_X, train_y.to_numpy(), batch_size=200, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 2ms/step - loss: 0.9312 - accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9311954379081726, 0.6000000238418579]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 시 loss와 accuracy 변화 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "ax1.plot(history.history['loss'], 'r-', label='train_loss')\n",
    "ax1.set_ylabel('train loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(history.history['accuracy'], 'g:', label='train_acc')\n",
    "ax2.set_ylabel('train acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c726b45dd7293eb31a34b8969b40a947eb51be7b196d382e5dff74d4c7ac181"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('saltlux_deep_lecture': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
